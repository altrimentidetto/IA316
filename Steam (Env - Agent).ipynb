{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steam (Environment - Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "# Basic import\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "from scipy.stats import invgamma, gamma\n",
    "from scipy.stats import t as student\n",
    "import pdb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout, Dot, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\" \n",
    "    Contextual Multi-Armed Bandit environment\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_films, nb_users, \n",
    "                 context_size = 2,\n",
    "                 displayed_users_embedding_size = 2, #used for the features vector\n",
    "                 displayed_games_embedding_size = 2, #used for the features vector\n",
    "                 noise_size = 3,\n",
    "                 seed=None):     \n",
    "        \n",
    "        self._rng = np.random.RandomState(seed)\n",
    "        #-------------------------------------------------------#\n",
    "        \n",
    "        self._nb_games = nb_games\n",
    "        self._nb_users = nb_users\n",
    "        self._p = context_size # size of user, size of game\n",
    "        self._displayed_users_embedding_size = displayed_users_embedding_size\n",
    "        self._displayed_games_embedding_size = displayed_games_embedding_size\n",
    "        self._noise_size = noise_size\n",
    "   \n",
    "        #-------------------------------------------------------#\n",
    "    \n",
    "        self.user_mean = np.ones(self._p)\n",
    "        self.user_var = np.ones(self._p)\n",
    "        self.game_mean = np.ones(self._p)\n",
    "        self.game_var = np.ones(self._p)\n",
    "        \n",
    "        #-------------------------------------------------------#\n",
    "        self.finish = False # flag to know when reset the environment (all games played)\n",
    "    \n",
    "    def step(self):\n",
    "        \n",
    "        if self.finish:\n",
    "            print(\"All games played reset the environment\")\n",
    "            \n",
    "        \"\"\" Choose a game \"\"\"\n",
    "        user = self._rng.randint(0, self._nb_users)\n",
    "        available_games = np.where(self._available_games[user] == 1)[0]\n",
    "        return user, available_games\n",
    "    \n",
    "    def update(self, user, game):\n",
    "        reward = self._reward_matrix[user, game]\n",
    "        self._available_games[user, game] = 0\n",
    "        return reward\n",
    "    \n",
    "    def reset(self):\n",
    "        #self._games = self._rng.uniform(size=(nb_games, context_size))\n",
    "        #self._users = self._rng.uniform(size=(nb_users, context_size))\n",
    "        #-------------------------------------------------------#\n",
    "\n",
    "        self._users = self._rng.normal(loc=self.user_mean,\n",
    "                                                scale=self.user_var,\n",
    "                                                size=(self._nb_users, self._p))\n",
    "        self._games = self._rng.normal(loc=self.game_mean,\n",
    "                                                scale=self.game_var,\n",
    "                                                size=(self._nb_games, self._p))\n",
    "        #-------------------------------------------------------#\n",
    "        self._reward_matrix = np.zeros((nb_users, nb_games))\n",
    "        \n",
    "        for i in range(self._reward_matrix.shape[0]):\n",
    "            for j in range(self._reward_matrix.shape[1]):\n",
    "                # reward = np.linalg.norm(self._games[j] - self._users[i], ord=2)\n",
    "                reward = self._games[j].dot(self._users[i])\n",
    "                self._reward_matrix[i, j] = reward\n",
    "                \n",
    "        self._reward_matrix = ((self._reward_matrix - np.min(self._reward_matrix)) / np.max(self._reward_matrix) * 4).astype(int) + 1\n",
    "        \n",
    "        self._available_games = np.ones((nb_users, nb_games))\n",
    "        self._available_films = np.ones((nb_users, nb_games))\n",
    "        \n",
    "        users = deepcopy(self._users)\n",
    "        \n",
    "        return users\n",
    "\n",
    "    def get_feature_vector(self, user, game):\n",
    "        user_embedding = self._users[user]\n",
    "        game_embedding = self._games[game]\n",
    "        \n",
    "        if self._displayed_users_embedding_size + self._displayed_games_embedding_size > 0:\n",
    "            variables = np.array([user_embedding[:self._displayed_users_embedding_size],\n",
    "                                  game_embedding[:self._displayed_games_embedding_size]])\n",
    "\n",
    "            if self._noise_size > 0:\n",
    "                noise = self._rng.normal(loc=np.ones(self._noise_size),\n",
    "                                         scale=np.ones(self._noise_size),\n",
    "                                         size=self._noise_size)\n",
    "                \n",
    "                variables = np.append(variables, noise)\n",
    "                \n",
    "            return variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \"\"\" \n",
    "    Random agent\n",
    "    \"\"\"\n",
    "    def __init__(self, seed = None):\n",
    "        self._rng = np.random.RandomState(seed)\n",
    "    \n",
    "    def act(self, available_games):\n",
    "        action = self._rng.choice(available_games)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic parameter\n",
    "nb_users = 30 #number of users in the context\n",
    "nb_games = 10 #number of games in the context\n",
    "context_size = 2 #number of different film categories = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.07139341,  2.6173452 ],\n",
       "       [ 0.10004728,  0.18273103],\n",
       "       [ 2.69541486,  0.41168055],\n",
       "       [ 0.07446554,  2.36395513],\n",
       "       [ 1.20953751, -1.54659002],\n",
       "       [ 1.69574011,  1.47757504],\n",
       "       [ 0.15213019,  2.02126636],\n",
       "       [ 1.74274769,  0.50308044],\n",
       "       [ 1.09446731, -1.93724045],\n",
       "       [ 0.52751627,  1.4322206 ],\n",
       "       [-0.26069544,  1.35101617],\n",
       "       [ 1.37336244, -0.88530617],\n",
       "       [ 1.64315708,  0.41449447],\n",
       "       [ 0.99366519,  1.19960656],\n",
       "       [ 0.19233297,  2.98700062],\n",
       "       [ 1.65652348,  0.68586502],\n",
       "       [ 1.30794171, -0.17118737],\n",
       "       [ 1.47940848,  2.01768386],\n",
       "       [ 0.14115165,  0.89107541],\n",
       "       [ 1.91547695,  0.33346789],\n",
       "       [ 0.88550255, -0.37891624],\n",
       "       [ 2.55044876,  0.72493372],\n",
       "       [ 0.20285665,  1.84067217],\n",
       "       [ 0.66825057,  0.73866055],\n",
       "       [ 2.72554292,  2.97795525],\n",
       "       [ 2.37740842,  1.13198666],\n",
       "       [-0.08376984, -0.53701157],\n",
       "       [ 1.90000765,  0.42722186],\n",
       "       [ 0.38593846,  2.04147271],\n",
       "       [ 2.12300997,  2.31930355]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the environment\n",
    "env = Environment(nb_games,nb_users,context_size,2020)\n",
    "env.reset() #reset and initilize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the agent\n",
    "agent = RandomAgent(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the experiment and generate some historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating matrix: \n",
      " [[0. 4. 2. 0. 2. 3. 0. 4. 0. 0.]\n",
      " [0. 0. 0. 0. 2. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 2. 0. 0. 4. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 3. 0. 0. 0. 3. 0.]\n",
      " [1. 0. 0. 0. 1. 0. 0. 0. 0. 2.]\n",
      " [3. 4. 2. 2. 0. 0. 0. 0. 0. 1.]\n",
      " [2. 0. 2. 1. 2. 2. 0. 3. 3. 1.]\n",
      " [0. 0. 1. 2. 1. 0. 3. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 2. 0. 0. 0.]\n",
      " [0. 0. 2. 0. 2. 2. 0. 3. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 2. 0. 0. 0.]\n",
      " [2. 2. 0. 0. 0. 0. 0. 0. 0. 2.]\n",
      " [2. 0. 1. 0. 1. 4. 3. 0. 3. 0.]\n",
      " [0. 3. 0. 0. 2. 0. 3. 3. 0. 0.]\n",
      " [3. 0. 0. 1. 0. 2. 0. 3. 3. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 2. 0. 0. 3. 0. 2. 2.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 4. 3. 1.]\n",
      " [0. 2. 2. 0. 2. 2. 0. 0. 2. 0.]\n",
      " [0. 3. 0. 0. 0. 0. 3. 0. 3. 0.]\n",
      " [2. 0. 0. 2. 0. 0. 2. 0. 0. 0.]\n",
      " [0. 4. 1. 2. 0. 0. 4. 4. 3. 2.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 2. 0. 2. 0. 0. 2. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 4. 0. 0.]\n",
      " [1. 0. 2. 2. 0. 0. 0. 0. 1. 2.]\n",
      " [0. 3. 0. 2. 0. 0. 0. 3. 0. 0.]\n",
      " [2. 0. 0. 0. 2. 0. 0. 0. 0. 0.]\n",
      " [3. 5. 0. 0. 2. 0. 4. 0. 4. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Running several trials\n",
    "nb_iteration = 100 #how many trials\n",
    "rating_matrix = np.zeros((env._nb_users, env._nb_games))\n",
    "users = list()\n",
    "games = list()\n",
    "ratings = list()\n",
    "for i in range(nb_iteration):\n",
    "    user, available_games = env.step()\n",
    "    choosen_game = agent.act(available_games)\n",
    "    reward = env.update(user, choosen_game)\n",
    "    users.append(user)\n",
    "    games.append(choosen_game)\n",
    "    ratings.append(reward)\n",
    "    rating_matrix[user, choosen_game] = reward\n",
    "    '''\n",
    "    print(\"user = {}, recommended_games = {}, choosen_game = {}\".format(user,recommended_games,choosen_game))\n",
    "    print(\"reward = {}\\n\".format(reward))\n",
    "    '''\n",
    "    \n",
    "print(\"rating matrix: \\n\", str(rating_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(Model):\n",
    "    def __init__(self, embedding_size, max_user, max_game):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.user_embedding = Embedding(output_dim=embedding_size,\n",
    "                                        input_dim=max_user,\n",
    "                                        input_length=1,\n",
    "                                        name='user_embedding')\n",
    "        self.game_embedding = Embedding(output_dim=embedding_size,\n",
    "                                        input_dim=max_game,\n",
    "                                        input_length=1,\n",
    "                                        name='game_embedding')\n",
    "        self.flatten = Flatten()\n",
    "        self.dot = Dot(axes=1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        user_inputs = inputs[0]\n",
    "        game_inputs = inputs[1]\n",
    "        \n",
    "        user_vecs = self.flatten(self.user_embedding(user_inputs))\n",
    "        game_vecs = self.flatten(self.game_embedding(game_inputs))\n",
    "        \n",
    "        y = self.dot([user_vecs, game_vecs])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepRegressionModel(Model):\n",
    "\n",
    "    def __init__(self, embedding_size, max_user, max_game):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.user_embedding = Embedding(output_dim=embedding_size,\n",
    "                                        input_dim=max_user,\n",
    "                                        input_length=1,\n",
    "                                        name='user_embedding')\n",
    "        self.game_embedding = Embedding(output_dim=embedding_size,\n",
    "                                        input_dim=max_game,\n",
    "                                        input_length=1,\n",
    "                                        name='game_embedding')\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        self.concat = Concatenate()\n",
    "        \n",
    "        self.dense1 = Dense(16, activation=\"relu\")\n",
    "        self.dense2 = Dense(8, activation=\"relu\")\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        user_inputs = inputs[0]\n",
    "        game_inputs = inputs[1]\n",
    "        feature_inputs = inputs[2]\n",
    "        \n",
    "        user_vecs = self.flatten(self.user_embedding(user_inputs))\n",
    "        game_vecs = self.flatten(self.game_embedding(game_inputs))\n",
    "        \n",
    "        # input_vecs = self.concat([user_vecs, game_vecs, self.flatten(feature_inputs)])\n",
    "        input_vecs = self.concat([user_vecs, game_vecs])\n",
    "        \n",
    "        y = self.dense1(input_vecs)\n",
    "        y = self.dense2(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingAgent:\n",
    "    def __init__(self, X, Y, deepRegression=False):\n",
    "        if deepRegression:\n",
    "            self._model = DeepRegressionModel(64, nb_users, nb_games)\n",
    "        else:\n",
    "            self._model = RegressionModel(64, nb_users, nb_games)\n",
    "        self._model.compile(optimizer=\"adam\", loss='mae')\n",
    "        self._model.fit(X, Y,\n",
    "                  batch_size=64, epochs=100, validation_split=0.1,\n",
    "                  shuffle=True)\n",
    "        self._user_embeddings = self._model.get_weights()[0]\n",
    "        self._game_embeddings = self._model.get_weights()[1]\n",
    "    \n",
    "    def act(self, user, available_games):\n",
    "        user_embedding = self._user_embeddings[user]\n",
    "        dot_products = self._game_embeddings @ user_embedding\n",
    "        user_embedding_norm = np.linalg.norm(user_embedding)\n",
    "        all_item_norms = np.linalg.norm(self._game_embeddings, axis=1)\n",
    "        norm_products = user_embedding_norm * all_item_norms\n",
    "        sims = dot_products / (norm_products)\n",
    "        sims = np.argsort(sims)[::-1]\n",
    "        mask = np.in1d(sims, available_games)\n",
    "        sims = sims[mask]\n",
    "        return sims[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90 samples, validate on 10 samples\n",
      "Epoch 1/100\n",
      "90/90 [==============================] - 0s 796us/sample - loss: 2.3558 - val_loss: 2.4010\n",
      "Epoch 2/100\n",
      "90/90 [==============================] - 0s 475us/sample - loss: 2.3534 - val_loss: 2.4010\n",
      "Epoch 3/100\n",
      "90/90 [==============================] - 0s 463us/sample - loss: 2.3514 - val_loss: 2.4011\n",
      "Epoch 4/100\n",
      "90/90 [==============================] - 0s 488us/sample - loss: 2.3495 - val_loss: 2.4011\n",
      "Epoch 5/100\n",
      "90/90 [==============================] - 0s 480us/sample - loss: 2.3476 - val_loss: 2.4011\n",
      "Epoch 6/100\n",
      "90/90 [==============================] - 0s 414us/sample - loss: 2.3456 - val_loss: 2.4010\n",
      "Epoch 7/100\n",
      "90/90 [==============================] - 0s 503us/sample - loss: 2.3436 - val_loss: 2.4008\n",
      "Epoch 8/100\n",
      "90/90 [==============================] - 0s 480us/sample - loss: 2.3415 - val_loss: 2.4006\n",
      "Epoch 9/100\n",
      "90/90 [==============================] - 0s 475us/sample - loss: 2.3393 - val_loss: 2.4002\n",
      "Epoch 10/100\n",
      "90/90 [==============================] - 0s 490us/sample - loss: 2.3370 - val_loss: 2.3998\n",
      "Epoch 11/100\n",
      "90/90 [==============================] - 0s 415us/sample - loss: 2.3346 - val_loss: 2.3993\n",
      "Epoch 12/100\n",
      "90/90 [==============================] - 0s 453us/sample - loss: 2.3320 - val_loss: 2.3986\n",
      "Epoch 13/100\n",
      "90/90 [==============================] - 0s 473us/sample - loss: 2.3292 - val_loss: 2.3978\n",
      "Epoch 14/100\n",
      "90/90 [==============================] - 0s 475us/sample - loss: 2.3263 - val_loss: 2.3969\n",
      "Epoch 15/100\n",
      "90/90 [==============================] - 0s 439us/sample - loss: 2.3231 - val_loss: 2.3958\n",
      "Epoch 16/100\n",
      "90/90 [==============================] - 0s 464us/sample - loss: 2.3197 - val_loss: 2.3945\n",
      "Epoch 17/100\n",
      "90/90 [==============================] - 0s 425us/sample - loss: 2.3161 - val_loss: 2.3931\n",
      "Epoch 18/100\n",
      "90/90 [==============================] - 0s 456us/sample - loss: 2.3122 - val_loss: 2.3915\n",
      "Epoch 19/100\n",
      "90/90 [==============================] - 0s 473us/sample - loss: 2.3080 - val_loss: 2.3898\n",
      "Epoch 20/100\n",
      "90/90 [==============================] - 0s 460us/sample - loss: 2.3035 - val_loss: 2.3878\n",
      "Epoch 21/100\n",
      "90/90 [==============================] - 0s 490us/sample - loss: 2.2986 - val_loss: 2.3856\n",
      "Epoch 22/100\n",
      "90/90 [==============================] - 0s 495us/sample - loss: 2.2934 - val_loss: 2.3831\n",
      "Epoch 23/100\n",
      "90/90 [==============================] - 0s 479us/sample - loss: 2.2878 - val_loss: 2.3803\n",
      "Epoch 24/100\n",
      "90/90 [==============================] - 0s 453us/sample - loss: 2.2818 - val_loss: 2.3773\n",
      "Epoch 25/100\n",
      "90/90 [==============================] - 0s 492us/sample - loss: 2.2754 - val_loss: 2.3739\n",
      "Epoch 26/100\n",
      "90/90 [==============================] - 0s 437us/sample - loss: 2.2686 - val_loss: 2.3703\n",
      "Epoch 27/100\n",
      "90/90 [==============================] - 0s 475us/sample - loss: 2.2613 - val_loss: 2.3664\n",
      "Epoch 28/100\n",
      "90/90 [==============================] - 0s 392us/sample - loss: 2.2535 - val_loss: 2.3620\n",
      "Epoch 29/100\n",
      "90/90 [==============================] - 0s 500us/sample - loss: 2.2452 - val_loss: 2.3574\n",
      "Epoch 30/100\n",
      "90/90 [==============================] - 0s 475us/sample - loss: 2.2364 - val_loss: 2.3523\n",
      "Epoch 31/100\n",
      "90/90 [==============================] - 0s 443us/sample - loss: 2.2270 - val_loss: 2.3468\n",
      "Epoch 32/100\n",
      "90/90 [==============================] - 0s 509us/sample - loss: 2.2171 - val_loss: 2.3408\n",
      "Epoch 33/100\n",
      "90/90 [==============================] - 0s 500us/sample - loss: 2.2065 - val_loss: 2.3343\n",
      "Epoch 34/100\n",
      "90/90 [==============================] - 0s 466us/sample - loss: 2.1952 - val_loss: 2.3273\n",
      "Epoch 35/100\n",
      "90/90 [==============================] - 0s 452us/sample - loss: 2.1835 - val_loss: 2.3199\n",
      "Epoch 36/100\n",
      "90/90 [==============================] - 0s 460us/sample - loss: 2.1710 - val_loss: 2.3120\n",
      "Epoch 37/100\n",
      "90/90 [==============================] - 0s 405us/sample - loss: 2.1578 - val_loss: 2.3036\n",
      "Epoch 38/100\n",
      "90/90 [==============================] - 0s 499us/sample - loss: 2.1439 - val_loss: 2.2945\n",
      "Epoch 39/100\n",
      "90/90 [==============================] - 0s 466us/sample - loss: 2.1294 - val_loss: 2.2849\n",
      "Epoch 40/100\n",
      "90/90 [==============================] - 0s 455us/sample - loss: 2.1141 - val_loss: 2.2747\n",
      "Epoch 41/100\n",
      "90/90 [==============================] - 0s 528us/sample - loss: 2.0980 - val_loss: 2.2642\n",
      "Epoch 42/100\n",
      "90/90 [==============================] - 0s 496us/sample - loss: 2.0812 - val_loss: 2.2532\n",
      "Epoch 43/100\n",
      "90/90 [==============================] - 0s 496us/sample - loss: 2.0636 - val_loss: 2.2416\n",
      "Epoch 44/100\n",
      "90/90 [==============================] - 0s 520us/sample - loss: 2.0452 - val_loss: 2.2294\n",
      "Epoch 45/100\n",
      "90/90 [==============================] - 0s 438us/sample - loss: 2.0261 - val_loss: 2.2167\n",
      "Epoch 46/100\n",
      "90/90 [==============================] - 0s 464us/sample - loss: 2.0060 - val_loss: 2.2033\n",
      "Epoch 47/100\n",
      "90/90 [==============================] - 0s 486us/sample - loss: 1.9850 - val_loss: 2.1891\n",
      "Epoch 48/100\n",
      "90/90 [==============================] - 0s 486us/sample - loss: 1.9634 - val_loss: 2.1744\n",
      "Epoch 49/100\n",
      "90/90 [==============================] - 0s 468us/sample - loss: 1.9409 - val_loss: 2.1589\n",
      "Epoch 50/100\n",
      "90/90 [==============================] - 0s 467us/sample - loss: 1.9173 - val_loss: 2.1424\n",
      "Epoch 51/100\n",
      "90/90 [==============================] - 0s 432us/sample - loss: 1.8928 - val_loss: 2.1251\n",
      "Epoch 52/100\n",
      "90/90 [==============================] - 0s 494us/sample - loss: 1.8677 - val_loss: 2.1071\n",
      "Epoch 53/100\n",
      "90/90 [==============================] - 0s 538us/sample - loss: 1.8414 - val_loss: 2.0885\n",
      "Epoch 54/100\n",
      "90/90 [==============================] - 0s 461us/sample - loss: 1.8143 - val_loss: 2.0690\n",
      "Epoch 55/100\n",
      "90/90 [==============================] - 0s 507us/sample - loss: 1.7862 - val_loss: 2.0488\n",
      "Epoch 56/100\n",
      "90/90 [==============================] - 0s 477us/sample - loss: 1.7572 - val_loss: 2.0280\n",
      "Epoch 57/100\n",
      "90/90 [==============================] - 0s 461us/sample - loss: 1.7274 - val_loss: 2.0060\n",
      "Epoch 58/100\n",
      "90/90 [==============================] - 0s 461us/sample - loss: 1.6964 - val_loss: 1.9832\n",
      "Epoch 59/100\n",
      "90/90 [==============================] - 0s 452us/sample - loss: 1.6645 - val_loss: 1.9597\n",
      "Epoch 60/100\n",
      "90/90 [==============================] - 0s 462us/sample - loss: 1.6316 - val_loss: 1.9352\n",
      "Epoch 61/100\n",
      "90/90 [==============================] - 0s 457us/sample - loss: 1.5977 - val_loss: 1.9100\n",
      "Epoch 62/100\n",
      "90/90 [==============================] - 0s 472us/sample - loss: 1.5628 - val_loss: 1.8840\n",
      "Epoch 63/100\n",
      "90/90 [==============================] - 0s 444us/sample - loss: 1.5273 - val_loss: 1.8569\n",
      "Epoch 64/100\n",
      "90/90 [==============================] - 0s 461us/sample - loss: 1.4923 - val_loss: 1.8288\n",
      "Epoch 65/100\n",
      "90/90 [==============================] - 0s 431us/sample - loss: 1.4579 - val_loss: 1.8004\n",
      "Epoch 66/100\n",
      "90/90 [==============================] - 0s 503us/sample - loss: 1.4245 - val_loss: 1.7717\n",
      "Epoch 67/100\n",
      "90/90 [==============================] - 0s 440us/sample - loss: 1.3922 - val_loss: 1.7433\n",
      "Epoch 68/100\n",
      "90/90 [==============================] - 0s 491us/sample - loss: 1.3608 - val_loss: 1.7148\n",
      "Epoch 69/100\n",
      "90/90 [==============================] - 0s 439us/sample - loss: 1.3290 - val_loss: 1.6860\n",
      "Epoch 70/100\n",
      "90/90 [==============================] - 0s 501us/sample - loss: 1.2990 - val_loss: 1.6570\n",
      "Epoch 71/100\n",
      "90/90 [==============================] - 0s 474us/sample - loss: 1.2698 - val_loss: 1.6277\n",
      "Epoch 72/100\n",
      "90/90 [==============================] - 0s 464us/sample - loss: 1.2405 - val_loss: 1.5983\n",
      "Epoch 73/100\n",
      "90/90 [==============================] - 0s 458us/sample - loss: 1.2137 - val_loss: 1.5688\n",
      "Epoch 74/100\n",
      "90/90 [==============================] - 0s 459us/sample - loss: 1.1838 - val_loss: 1.5392\n",
      "Epoch 75/100\n",
      "90/90 [==============================] - 0s 515us/sample - loss: 1.1552 - val_loss: 1.5091\n",
      "Epoch 76/100\n",
      "90/90 [==============================] - 0s 477us/sample - loss: 1.1242 - val_loss: 1.4786\n",
      "Epoch 77/100\n",
      "90/90 [==============================] - 0s 456us/sample - loss: 1.0953 - val_loss: 1.4480\n",
      "Epoch 78/100\n",
      "90/90 [==============================] - 0s 527us/sample - loss: 1.0630 - val_loss: 1.4172\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 492us/sample - loss: 1.0320 - val_loss: 1.3854\n",
      "Epoch 80/100\n",
      "90/90 [==============================] - 0s 463us/sample - loss: 0.9996 - val_loss: 1.3531\n",
      "Epoch 81/100\n",
      "90/90 [==============================] - 0s 390us/sample - loss: 0.9677 - val_loss: 1.3201\n",
      "Epoch 82/100\n",
      "90/90 [==============================] - 0s 488us/sample - loss: 0.9340 - val_loss: 1.2870\n",
      "Epoch 83/100\n",
      "90/90 [==============================] - 0s 409us/sample - loss: 0.9009 - val_loss: 1.2532\n",
      "Epoch 84/100\n",
      "90/90 [==============================] - 0s 379us/sample - loss: 0.8682 - val_loss: 1.2185\n",
      "Epoch 85/100\n",
      "90/90 [==============================] - 0s 502us/sample - loss: 0.8368 - val_loss: 1.1838\n",
      "Epoch 86/100\n",
      "90/90 [==============================] - 0s 485us/sample - loss: 0.8055 - val_loss: 1.1496\n",
      "Epoch 87/100\n",
      "90/90 [==============================] - 0s 459us/sample - loss: 0.7763 - val_loss: 1.1155\n",
      "Epoch 88/100\n",
      "90/90 [==============================] - 0s 489us/sample - loss: 0.7483 - val_loss: 1.0824\n",
      "Epoch 89/100\n",
      "90/90 [==============================] - 0s 423us/sample - loss: 0.7195 - val_loss: 1.0506\n",
      "Epoch 90/100\n",
      "90/90 [==============================] - 0s 500us/sample - loss: 0.6957 - val_loss: 1.0201\n",
      "Epoch 91/100\n",
      "90/90 [==============================] - 0s 466us/sample - loss: 0.6728 - val_loss: 0.9911\n",
      "Epoch 92/100\n",
      "90/90 [==============================] - 0s 454us/sample - loss: 0.6505 - val_loss: 0.9628\n",
      "Epoch 93/100\n",
      "90/90 [==============================] - 0s 424us/sample - loss: 0.6304 - val_loss: 0.9362\n",
      "Epoch 94/100\n",
      "90/90 [==============================] - 0s 399us/sample - loss: 0.6118 - val_loss: 0.9107\n",
      "Epoch 95/100\n",
      "90/90 [==============================] - 0s 491us/sample - loss: 0.5914 - val_loss: 0.8876\n",
      "Epoch 96/100\n",
      "90/90 [==============================] - 0s 421us/sample - loss: 0.5737 - val_loss: 0.8656\n",
      "Epoch 97/100\n",
      "90/90 [==============================] - 0s 479us/sample - loss: 0.5581 - val_loss: 0.8448\n",
      "Epoch 98/100\n",
      "90/90 [==============================] - 0s 452us/sample - loss: 0.5412 - val_loss: 0.8264\n",
      "Epoch 99/100\n",
      "90/90 [==============================] - 0s 446us/sample - loss: 0.5240 - val_loss: 0.8093\n",
      "Epoch 100/100\n",
      "90/90 [==============================] - 0s 432us/sample - loss: 0.5070 - val_loss: 0.7934\n"
     ]
    }
   ],
   "source": [
    "deepRegression = False\n",
    "\n",
    "users = np.array(users)\n",
    "games = np.array(games)\n",
    "ratings = np.array(ratings)\n",
    "\n",
    "if deepRegression:\n",
    "    features = []\n",
    "    for i in range(len(users)):\n",
    "        features.append(env.get_feature_vector(users[i], games[i]))\n",
    "    features = np.float64(features)\n",
    "    agent = EmbeddingAgent([users, games, features], ratings, deepRegression=deepRegression)\n",
    "else:\n",
    "    agent = EmbeddingAgent([users, games], ratings, deepRegression=deepRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user = 24, available games = [0 1 2 4 5 6 7 8 9], choosen_game = 9\n",
      "reward = 1\n",
      "\n",
      "user = 7, available games = [0 1 5 7 8 9], choosen_game = 0\n",
      "reward = 2\n",
      "\n",
      "user = 22, available games = [0 1 2 3 4 5 6 7 8 9], choosen_game = 7\n",
      "reward = 3\n",
      "\n",
      "user = 2, available games = [0 1 2 4 5 7 8 9], choosen_game = 0\n",
      "reward = 3\n",
      "\n",
      "user = 6, available games = [1 6], choosen_game = 1\n",
      "reward = 3\n",
      "\n",
      "user = 4, available games = [1 2 3 5 6 7 8], choosen_game = 3\n",
      "reward = 2\n",
      "\n",
      "user = 11, available games = [2 3 4 5 6 7 8], choosen_game = 3\n",
      "reward = 2\n",
      "\n",
      "user = 21, available games = [0 4 5], choosen_game = 0\n",
      "reward = 3\n",
      "\n",
      "user = 21, available games = [4 5], choosen_game = 5\n",
      "reward = 5\n",
      "\n",
      "user = 26, available games = [1 4 5 6 7], choosen_game = 6\n",
      "reward = 1\n",
      "\n",
      "user = 23, available games = [0 1 2 4 6 7 9], choosen_game = 6\n",
      "reward = 2\n",
      "\n",
      "user = 8, available games = [0 1 2 3 4 5 7 8 9], choosen_game = 3\n",
      "reward = 2\n",
      "\n",
      "user = 9, available games = [0 1 3 6 8 9], choosen_game = 1\n",
      "reward = 3\n",
      "\n",
      "user = 16, available games = [0 1 2 4 5 7], choosen_game = 0\n",
      "reward = 2\n",
      "\n",
      "user = 14, available games = [1 2 4 6 9], choosen_game = 1\n",
      "reward = 4\n",
      "\n",
      "user = 26, available games = [1 4 5 7], choosen_game = 1\n",
      "reward = 1\n",
      "\n",
      "user = 23, available games = [0 1 2 4 7 9], choosen_game = 0\n",
      "reward = 2\n",
      "\n",
      "user = 23, available games = [1 2 4 7 9], choosen_game = 1\n",
      "reward = 3\n",
      "\n",
      "user = 8, available games = [0 1 2 4 5 7 8 9], choosen_game = 8\n",
      "reward = 1\n",
      "\n",
      "user = 23, available games = [2 4 7 9], choosen_game = 9\n",
      "reward = 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running several trials\n",
    "nb_iteration = 20 #how many trials\n",
    "for i in range(nb_iteration):\n",
    "    user, available_games = env.step()\n",
    "    choosen_game = agent.act(user, available_games)\n",
    "    reward = env.update(user, choosen_game)\n",
    "    rating_matrix[user, choosen_game] = reward\n",
    "    print(\"user = {}, available games = {}, choosen_game = {}\".format(user,available_games,choosen_game))\n",
    "    print(\"reward = {}\\n\".format(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
