{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steam (Environment - Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "# Basic import\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "from scipy.stats import invgamma, gamma\n",
    "from scipy.stats import t as student\n",
    "import pdb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout, Dot, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\" \n",
    "    Contextual Multi-Armed Bandit environment\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_films, nb_users, \n",
    "                 context_size = 2,\n",
    "                 displayed_users_embedding_size = 2, #used for the features vector\n",
    "                 displayed_games_embedding_size = 2, #used for the features vector\n",
    "                 noise_size = 3,\n",
    "                 seed=None):     \n",
    "        \n",
    "        self._rng = np.random.RandomState(seed)\n",
    "        #-------------------------------------------------------#\n",
    "        \n",
    "        self._nb_games = nb_games\n",
    "        self._nb_users = nb_users\n",
    "        self._p = context_size # size of user, size of game\n",
    "        self._displayed_users_embedding_size = displayed_users_embedding_size\n",
    "        self._displayed_games_embedding_size = displayed_games_embedding_size\n",
    "        self._noise_size = noise_size\n",
    "   \n",
    "        #-------------------------------------------------------#\n",
    "    \n",
    "        self.user_mean = np.ones(self._p)\n",
    "        self.user_var = np.ones(self._p)\n",
    "        self.game_mean = np.ones(self._p)\n",
    "        self.game_var = np.ones(self._p)\n",
    "        \n",
    "        #-------------------------------------------------------#\n",
    "        self.finish = False # flag to know when reset the environment (all games played)\n",
    "    \n",
    "    def step(self):\n",
    "        \n",
    "        if self.finish:\n",
    "            print(\"All games played reset the environment\")\n",
    "            \n",
    "        \"\"\" Choose a game \"\"\"\n",
    "        user = self._rng.randint(0, self._nb_users)\n",
    "        available_games = np.where(self._available_games[user] == 1)[0]\n",
    "        return user, available_games\n",
    "    \n",
    "    def update(self, user, game):\n",
    "        reward = self._reward_matrix[user, game]\n",
    "        self._available_games[user, game] = 0\n",
    "        return reward\n",
    "    \n",
    "    def reset(self):\n",
    "        #self._games = self._rng.uniform(size=(nb_games, context_size))\n",
    "        #self._users = self._rng.uniform(size=(nb_users, context_size))\n",
    "        #-------------------------------------------------------#\n",
    "\n",
    "        self._users = self._rng.normal(loc=self.user_mean,\n",
    "                                                scale=self.user_var,\n",
    "                                                size=(self._nb_users, self._p))\n",
    "        self._games = self._rng.normal(loc=self.game_mean,\n",
    "                                                scale=self.game_var,\n",
    "                                                size=(self._nb_games, self._p))\n",
    "        #-------------------------------------------------------#\n",
    "        self._reward_matrix = np.zeros((nb_users, nb_games))\n",
    "        \n",
    "        for i in range(self._reward_matrix.shape[0]):\n",
    "            for j in range(self._reward_matrix.shape[1]):\n",
    "                # reward = np.linalg.norm(self._games[j] - self._users[i], ord=2)\n",
    "                reward = self._games[j].dot(self._users[i])\n",
    "                self._reward_matrix[i, j] = reward\n",
    "                \n",
    "        self._reward_matrix = ((self._reward_matrix - np.min(self._reward_matrix)) / np.max(self._reward_matrix) * 4).astype(int) + 1\n",
    "        \n",
    "        self._available_games = np.ones((nb_users, nb_games))\n",
    "        self._available_films = np.ones((nb_users, nb_games))\n",
    "        \n",
    "        users = deepcopy(self._users)\n",
    "        \n",
    "        return users\n",
    "\n",
    "    def get_feature_vector(self, user, game):\n",
    "        user_embedding = self._users[user]\n",
    "        game_embedding = self._games[game]\n",
    "        \n",
    "        if self._displayed_users_embedding_size + self._displayed_games_embedding_size > 0:\n",
    "            variables = np.array([user_embedding[:self._displayed_users_embedding_size],\n",
    "                                  game_embedding[:self._displayed_games_embedding_size]])\n",
    "\n",
    "            if self._noise_size > 0:\n",
    "                noise = self._rng.normal(loc=np.ones(self._noise_size),\n",
    "                                         scale=np.ones(self._noise_size),\n",
    "                                         size=self._noise_size)\n",
    "                \n",
    "                variables = np.append(variables, noise)\n",
    "                \n",
    "            return variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \"\"\" \n",
    "    Random agent\n",
    "    \"\"\"\n",
    "    def __init__(self, seed = None):\n",
    "        self._rng = np.random.RandomState(seed)\n",
    "    \n",
    "    def act(self, available_games):\n",
    "        action = self._rng.choice(available_games)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic parameter\n",
    "nb_users = 30 #number of users in the context\n",
    "nb_games = 10 #number of games in the context\n",
    "context_size = 2 #number of different film categories = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.07139341,  2.6173452 ],\n",
       "       [ 0.10004728,  0.18273103],\n",
       "       [ 2.69541486,  0.41168055],\n",
       "       [ 0.07446554,  2.36395513],\n",
       "       [ 1.20953751, -1.54659002],\n",
       "       [ 1.69574011,  1.47757504],\n",
       "       [ 0.15213019,  2.02126636],\n",
       "       [ 1.74274769,  0.50308044],\n",
       "       [ 1.09446731, -1.93724045],\n",
       "       [ 0.52751627,  1.4322206 ],\n",
       "       [-0.26069544,  1.35101617],\n",
       "       [ 1.37336244, -0.88530617],\n",
       "       [ 1.64315708,  0.41449447],\n",
       "       [ 0.99366519,  1.19960656],\n",
       "       [ 0.19233297,  2.98700062],\n",
       "       [ 1.65652348,  0.68586502],\n",
       "       [ 1.30794171, -0.17118737],\n",
       "       [ 1.47940848,  2.01768386],\n",
       "       [ 0.14115165,  0.89107541],\n",
       "       [ 1.91547695,  0.33346789],\n",
       "       [ 0.88550255, -0.37891624],\n",
       "       [ 2.55044876,  0.72493372],\n",
       "       [ 0.20285665,  1.84067217],\n",
       "       [ 0.66825057,  0.73866055],\n",
       "       [ 2.72554292,  2.97795525],\n",
       "       [ 2.37740842,  1.13198666],\n",
       "       [-0.08376984, -0.53701157],\n",
       "       [ 1.90000765,  0.42722186],\n",
       "       [ 0.38593846,  2.04147271],\n",
       "       [ 2.12300997,  2.31930355]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the environment\n",
    "env = Environment(nb_games,nb_users,context_size,2020)\n",
    "env.reset() #reset and initilize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the agent\n",
    "agent = RandomAgent(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the experiment and generate some historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating matrix: \n",
      " [[0. 4. 2. 0. 2. 3. 0. 4. 0. 0.]\n",
      " [0. 0. 0. 0. 2. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 2. 0. 0. 4. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 3. 0. 0. 0. 3. 0.]\n",
      " [1. 0. 0. 0. 1. 0. 0. 0. 0. 2.]\n",
      " [3. 4. 2. 2. 0. 0. 0. 0. 0. 1.]\n",
      " [2. 0. 2. 1. 2. 2. 0. 3. 3. 1.]\n",
      " [0. 0. 1. 2. 1. 0. 3. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 2. 0. 0. 0.]\n",
      " [0. 0. 2. 0. 2. 2. 0. 3. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 2. 0. 0. 0.]\n",
      " [2. 2. 0. 0. 0. 0. 0. 0. 0. 2.]\n",
      " [2. 0. 1. 0. 1. 4. 3. 0. 3. 0.]\n",
      " [0. 3. 0. 0. 2. 0. 3. 3. 0. 0.]\n",
      " [3. 0. 0. 1. 0. 2. 0. 3. 3. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 2. 0. 0. 3. 0. 2. 2.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 4. 3. 1.]\n",
      " [0. 2. 2. 0. 2. 2. 0. 0. 2. 0.]\n",
      " [0. 3. 0. 0. 0. 0. 3. 0. 3. 0.]\n",
      " [2. 0. 0. 2. 0. 0. 2. 0. 0. 0.]\n",
      " [0. 4. 1. 2. 0. 0. 4. 4. 3. 2.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 2. 0. 2. 0. 0. 2. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 4. 0. 0.]\n",
      " [1. 0. 2. 2. 0. 0. 0. 0. 1. 2.]\n",
      " [0. 3. 0. 2. 0. 0. 0. 3. 0. 0.]\n",
      " [2. 0. 0. 0. 2. 0. 0. 0. 0. 0.]\n",
      " [3. 5. 0. 0. 2. 0. 4. 0. 4. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Running several trials\n",
    "nb_iteration = 100 #how many trials\n",
    "rating_matrix = np.zeros((env._nb_users, env._nb_games))\n",
    "users = list()\n",
    "games = list()\n",
    "ratings = list()\n",
    "for i in range(nb_iteration):\n",
    "    user, available_games = env.step()\n",
    "    choosen_game = agent.act(available_games)\n",
    "    reward = env.update(user, choosen_game)\n",
    "    users.append(user)\n",
    "    games.append(choosen_game)\n",
    "    ratings.append(reward)\n",
    "    rating_matrix[user, choosen_game] = reward\n",
    "    '''\n",
    "    print(\"user = {}, recommended_games = {}, choosen_game = {}\".format(user,recommended_games,choosen_game))\n",
    "    print(\"reward = {}\\n\".format(reward))\n",
    "    '''\n",
    "    \n",
    "print(\"rating matrix: \\n\", str(rating_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(Model):\n",
    "    def __init__(self, embedding_size, max_user, max_game):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.user_embedding = Embedding(output_dim=embedding_size,\n",
    "                                        input_dim=max_user,\n",
    "                                        input_length=1,\n",
    "                                        name='user_embedding')\n",
    "        self.game_embedding = Embedding(output_dim=embedding_size,\n",
    "                                        input_dim=max_game,\n",
    "                                        input_length=1,\n",
    "                                        name='game_embedding')\n",
    "        self.flatten = Flatten()\n",
    "        self.dot = Dot(axes=1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        user_inputs = inputs[0]\n",
    "        game_inputs = inputs[1]\n",
    "        \n",
    "        user_vecs = self.flatten(self.user_embedding(user_inputs))\n",
    "        game_vecs = self.flatten(self.game_embedding(game_inputs))\n",
    "        \n",
    "        y = self.dot([user_vecs, game_vecs])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepRegressionModel(Model):\n",
    "\n",
    "    def __init__(self, embedding_size, max_user, max_game):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.user_embedding = Embedding(output_dim=embedding_size,\n",
    "                                        input_dim=max_user,\n",
    "                                        input_length=1,\n",
    "                                        name='user_embedding')\n",
    "        self.game_embedding = Embedding(output_dim=embedding_size,\n",
    "                                        input_dim=max_game,\n",
    "                                        input_length=1,\n",
    "                                        name='game_embedding')\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        self.concat = Concatenate()\n",
    "        \n",
    "        self.dense1 = Dense(16, activation=\"relu\")\n",
    "        self.dense2 = Dense(8, activation=\"relu\")\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        user_inputs = inputs[0]\n",
    "        game_inputs = inputs[1]\n",
    "        feature_inputs = inputs[2]\n",
    "        \n",
    "        user_vecs = self.flatten(self.user_embedding(user_inputs))\n",
    "        game_vecs = self.flatten(self.game_embedding(game_inputs))\n",
    "        \n",
    "        # input_vecs = self.concat([user_vecs, game_vecs, self.flatten(feature_inputs)])\n",
    "        input_vecs = self.concat([user_vecs, game_vecs])\n",
    "        \n",
    "        y = self.dense1(input_vecs)\n",
    "        y = self.dense2(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingAgent:\n",
    "    def __init__(self, X, Y, deepRegression=False):\n",
    "        if deepRegression:\n",
    "            self._model = DeepRegressionModel(64, nb_users, nb_games)\n",
    "        else:\n",
    "            self._model = RegressionModel(64, nb_users, nb_games)\n",
    "        self._model.compile(optimizer=\"adam\", loss='mae')\n",
    "        self._model.fit(X, Y,\n",
    "                  batch_size=64, epochs=100, validation_split=0.1,\n",
    "                  shuffle=True)\n",
    "        self._user_embeddings = self._model.get_weights()['user_embedding']\n",
    "        self._game_embeddings = self._model.get_weights()['game_embedding']\n",
    "    \n",
    "    def act(self, user, available_games):\n",
    "        user_embedding = self._user_embeddings[user]\n",
    "        dot_products = self._game_embeddings @ user_embedding\n",
    "        user_embedding_norm = np.linalg.norm(user_embedding)\n",
    "        all_item_norms = np.linalg.norm(self._game_embeddings, axis=1)\n",
    "        norm_products = user_embedding_norm * all_item_norms\n",
    "        sims = dot_products / (norm_products)\n",
    "        sims = np.argsort(sims)[::-1]\n",
    "        mask = np.in1d(sims, available_games)\n",
    "        sims = sims[mask]\n",
    "        return sims[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90 samples, validate on 10 samples\n",
      "Epoch 1/100\n",
      "90/90 [==============================] - 1s 8ms/sample - loss: 2.3339 - val_loss: 2.3706\n",
      "Epoch 2/100\n",
      "90/90 [==============================] - 0s 541us/sample - loss: 2.3216 - val_loss: 2.3600\n",
      "Epoch 3/100\n",
      "90/90 [==============================] - 0s 530us/sample - loss: 2.3082 - val_loss: 2.3479\n",
      "Epoch 4/100\n",
      "90/90 [==============================] - 0s 471us/sample - loss: 2.2938 - val_loss: 2.3343\n",
      "Epoch 5/100\n",
      "90/90 [==============================] - 0s 527us/sample - loss: 2.2783 - val_loss: 2.3198\n",
      "Epoch 6/100\n",
      "90/90 [==============================] - 0s 608us/sample - loss: 2.2615 - val_loss: 2.3041\n",
      "Epoch 7/100\n",
      "90/90 [==============================] - 0s 604us/sample - loss: 2.2434 - val_loss: 2.2869\n",
      "Epoch 8/100\n",
      "90/90 [==============================] - 0s 459us/sample - loss: 2.2237 - val_loss: 2.2677\n",
      "Epoch 9/100\n",
      "90/90 [==============================] - 0s 541us/sample - loss: 2.2021 - val_loss: 2.2462\n",
      "Epoch 10/100\n",
      "90/90 [==============================] - 0s 622us/sample - loss: 2.1781 - val_loss: 2.2219\n",
      "Epoch 11/100\n",
      "90/90 [==============================] - 0s 549us/sample - loss: 2.1516 - val_loss: 2.1950\n",
      "Epoch 12/100\n",
      "90/90 [==============================] - 0s 571us/sample - loss: 2.1223 - val_loss: 2.1650\n",
      "Epoch 13/100\n",
      "90/90 [==============================] - 0s 592us/sample - loss: 2.0901 - val_loss: 2.1319\n",
      "Epoch 14/100\n",
      "90/90 [==============================] - 0s 518us/sample - loss: 2.0546 - val_loss: 2.0955\n",
      "Epoch 15/100\n",
      "90/90 [==============================] - 0s 485us/sample - loss: 2.0151 - val_loss: 2.0552\n",
      "Epoch 16/100\n",
      "90/90 [==============================] - 0s 614us/sample - loss: 1.9716 - val_loss: 2.0106\n",
      "Epoch 17/100\n",
      "90/90 [==============================] - 0s 472us/sample - loss: 1.9236 - val_loss: 1.9614\n",
      "Epoch 18/100\n",
      "90/90 [==============================] - 0s 599us/sample - loss: 1.8711 - val_loss: 1.9090\n",
      "Epoch 19/100\n",
      "90/90 [==============================] - 0s 611us/sample - loss: 1.8157 - val_loss: 1.8543\n",
      "Epoch 20/100\n",
      "90/90 [==============================] - 0s 504us/sample - loss: 1.7587 - val_loss: 1.7982\n",
      "Epoch 21/100\n",
      "90/90 [==============================] - 0s 561us/sample - loss: 1.7038 - val_loss: 1.7377\n",
      "Epoch 22/100\n",
      "90/90 [==============================] - 0s 559us/sample - loss: 1.6432 - val_loss: 1.6724\n",
      "Epoch 23/100\n",
      "90/90 [==============================] - 0s 520us/sample - loss: 1.5820 - val_loss: 1.6037\n",
      "Epoch 24/100\n",
      "90/90 [==============================] - 0s 597us/sample - loss: 1.5203 - val_loss: 1.5327\n",
      "Epoch 25/100\n",
      "90/90 [==============================] - 0s 510us/sample - loss: 1.4562 - val_loss: 1.4566\n",
      "Epoch 26/100\n",
      "90/90 [==============================] - 0s 564us/sample - loss: 1.3879 - val_loss: 1.3759\n",
      "Epoch 27/100\n",
      "90/90 [==============================] - 0s 578us/sample - loss: 1.3232 - val_loss: 1.2965\n",
      "Epoch 28/100\n",
      "90/90 [==============================] - 0s 495us/sample - loss: 1.2664 - val_loss: 1.2187\n",
      "Epoch 29/100\n",
      "90/90 [==============================] - 0s 570us/sample - loss: 1.2225 - val_loss: 1.1480\n",
      "Epoch 30/100\n",
      "90/90 [==============================] - 0s 595us/sample - loss: 1.1834 - val_loss: 1.0924\n",
      "Epoch 31/100\n",
      "90/90 [==============================] - 0s 496us/sample - loss: 1.1501 - val_loss: 1.0490\n",
      "Epoch 32/100\n",
      "90/90 [==============================] - 0s 615us/sample - loss: 1.1142 - val_loss: 1.0115\n",
      "Epoch 33/100\n",
      "90/90 [==============================] - 0s 512us/sample - loss: 1.0803 - val_loss: 0.9752\n",
      "Epoch 34/100\n",
      "90/90 [==============================] - 0s 498us/sample - loss: 1.0491 - val_loss: 0.9411\n",
      "Epoch 35/100\n",
      "90/90 [==============================] - 0s 640us/sample - loss: 1.0177 - val_loss: 0.9134\n",
      "Epoch 36/100\n",
      "90/90 [==============================] - 0s 558us/sample - loss: 0.9884 - val_loss: 0.8894\n",
      "Epoch 37/100\n",
      "90/90 [==============================] - 0s 500us/sample - loss: 0.9583 - val_loss: 0.8642\n",
      "Epoch 38/100\n",
      "90/90 [==============================] - 0s 549us/sample - loss: 0.9280 - val_loss: 0.8385\n",
      "Epoch 39/100\n",
      "90/90 [==============================] - 0s 486us/sample - loss: 0.8990 - val_loss: 0.8122\n",
      "Epoch 40/100\n",
      "90/90 [==============================] - 0s 584us/sample - loss: 0.8705 - val_loss: 0.7854\n",
      "Epoch 41/100\n",
      "90/90 [==============================] - 0s 582us/sample - loss: 0.8442 - val_loss: 0.7596\n",
      "Epoch 42/100\n",
      "90/90 [==============================] - 0s 573us/sample - loss: 0.8217 - val_loss: 0.7361\n",
      "Epoch 43/100\n",
      "90/90 [==============================] - 0s 505us/sample - loss: 0.8012 - val_loss: 0.7162\n",
      "Epoch 44/100\n",
      "90/90 [==============================] - 0s 541us/sample - loss: 0.7824 - val_loss: 0.7007\n",
      "Epoch 45/100\n",
      "90/90 [==============================] - 0s 574us/sample - loss: 0.7653 - val_loss: 0.6859\n",
      "Epoch 46/100\n",
      "90/90 [==============================] - 0s 558us/sample - loss: 0.7484 - val_loss: 0.6749\n",
      "Epoch 47/100\n",
      "90/90 [==============================] - 0s 541us/sample - loss: 0.7346 - val_loss: 0.6705\n",
      "Epoch 48/100\n",
      "90/90 [==============================] - 0s 559us/sample - loss: 0.7215 - val_loss: 0.6688\n",
      "Epoch 49/100\n",
      "90/90 [==============================] - 0s 589us/sample - loss: 0.7095 - val_loss: 0.6719\n",
      "Epoch 50/100\n",
      "90/90 [==============================] - 0s 496us/sample - loss: 0.7000 - val_loss: 0.6796\n",
      "Epoch 51/100\n",
      "90/90 [==============================] - 0s 572us/sample - loss: 0.6939 - val_loss: 0.6875\n",
      "Epoch 52/100\n",
      "90/90 [==============================] - 0s 521us/sample - loss: 0.6883 - val_loss: 0.6903\n",
      "Epoch 53/100\n",
      "90/90 [==============================] - 0s 550us/sample - loss: 0.6817 - val_loss: 0.6880\n",
      "Epoch 54/100\n",
      "90/90 [==============================] - 0s 577us/sample - loss: 0.6775 - val_loss: 0.6854\n",
      "Epoch 55/100\n",
      "90/90 [==============================] - 0s 498us/sample - loss: 0.6730 - val_loss: 0.6851\n",
      "Epoch 56/100\n",
      "90/90 [==============================] - 0s 561us/sample - loss: 0.6703 - val_loss: 0.6896\n",
      "Epoch 57/100\n",
      "90/90 [==============================] - 0s 542us/sample - loss: 0.6668 - val_loss: 0.6941\n",
      "Epoch 58/100\n",
      "90/90 [==============================] - 0s 559us/sample - loss: 0.6640 - val_loss: 0.6990\n",
      "Epoch 59/100\n",
      "90/90 [==============================] - 0s 564us/sample - loss: 0.6615 - val_loss: 0.7045\n",
      "Epoch 60/100\n",
      "90/90 [==============================] - 0s 516us/sample - loss: 0.6585 - val_loss: 0.7095\n",
      "Epoch 61/100\n",
      "90/90 [==============================] - 0s 506us/sample - loss: 0.6553 - val_loss: 0.7158\n",
      "Epoch 62/100\n",
      "90/90 [==============================] - 0s 561us/sample - loss: 0.6537 - val_loss: 0.7224\n",
      "Epoch 63/100\n",
      "90/90 [==============================] - 0s 593us/sample - loss: 0.6514 - val_loss: 0.7253\n",
      "Epoch 64/100\n",
      "90/90 [==============================] - 0s 569us/sample - loss: 0.6499 - val_loss: 0.7249\n",
      "Epoch 65/100\n",
      "90/90 [==============================] - 0s 530us/sample - loss: 0.6485 - val_loss: 0.7249\n",
      "Epoch 66/100\n",
      "90/90 [==============================] - 0s 553us/sample - loss: 0.6471 - val_loss: 0.7258\n",
      "Epoch 67/100\n",
      "90/90 [==============================] - 0s 553us/sample - loss: 0.6458 - val_loss: 0.7290\n",
      "Epoch 68/100\n",
      "90/90 [==============================] - 0s 562us/sample - loss: 0.6447 - val_loss: 0.7334\n",
      "Epoch 69/100\n",
      "90/90 [==============================] - 0s 542us/sample - loss: 0.6440 - val_loss: 0.7381\n",
      "Epoch 70/100\n",
      "90/90 [==============================] - 0s 525us/sample - loss: 0.6427 - val_loss: 0.7421\n",
      "Epoch 71/100\n",
      "90/90 [==============================] - 0s 573us/sample - loss: 0.6416 - val_loss: 0.7459\n",
      "Epoch 72/100\n",
      "90/90 [==============================] - 0s 535us/sample - loss: 0.6408 - val_loss: 0.7487\n",
      "Epoch 73/100\n",
      "90/90 [==============================] - 0s 564us/sample - loss: 0.6401 - val_loss: 0.7507\n",
      "Epoch 74/100\n",
      "90/90 [==============================] - 0s 547us/sample - loss: 0.6393 - val_loss: 0.7534\n",
      "Epoch 75/100\n",
      "90/90 [==============================] - 0s 499us/sample - loss: 0.6388 - val_loss: 0.7564\n",
      "Epoch 76/100\n",
      "90/90 [==============================] - 0s 601us/sample - loss: 0.6380 - val_loss: 0.7598\n",
      "Epoch 77/100\n",
      "90/90 [==============================] - 0s 458us/sample - loss: 0.6375 - val_loss: 0.7621\n",
      "Epoch 78/100\n",
      "90/90 [==============================] - 0s 472us/sample - loss: 0.6363 - val_loss: 0.7620\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 478us/sample - loss: 0.6359 - val_loss: 0.7605\n",
      "Epoch 80/100\n",
      "90/90 [==============================] - 0s 476us/sample - loss: 0.6346 - val_loss: 0.7590\n",
      "Epoch 81/100\n",
      "90/90 [==============================] - 0s 510us/sample - loss: 0.6339 - val_loss: 0.7580\n",
      "Epoch 82/100\n",
      "90/90 [==============================] - 0s 501us/sample - loss: 0.6343 - val_loss: 0.7585\n",
      "Epoch 83/100\n",
      "90/90 [==============================] - 0s 495us/sample - loss: 0.6331 - val_loss: 0.7616\n",
      "Epoch 84/100\n",
      "90/90 [==============================] - 0s 491us/sample - loss: 0.6328 - val_loss: 0.7638\n",
      "Epoch 85/100\n",
      "90/90 [==============================] - 0s 500us/sample - loss: 0.6326 - val_loss: 0.7655\n",
      "Epoch 86/100\n",
      "90/90 [==============================] - 0s 495us/sample - loss: 0.6318 - val_loss: 0.7677\n",
      "Epoch 87/100\n",
      "90/90 [==============================] - 0s 512us/sample - loss: 0.6309 - val_loss: 0.7702\n",
      "Epoch 88/100\n",
      "90/90 [==============================] - 0s 475us/sample - loss: 0.6305 - val_loss: 0.7729\n",
      "Epoch 89/100\n",
      "90/90 [==============================] - 0s 550us/sample - loss: 0.6304 - val_loss: 0.7774\n",
      "Epoch 90/100\n",
      "90/90 [==============================] - 0s 521us/sample - loss: 0.6300 - val_loss: 0.7833\n",
      "Epoch 91/100\n",
      "90/90 [==============================] - 0s 525us/sample - loss: 0.6296 - val_loss: 0.7910\n",
      "Epoch 92/100\n",
      "90/90 [==============================] - 0s 502us/sample - loss: 0.6295 - val_loss: 0.7987\n",
      "Epoch 93/100\n",
      "90/90 [==============================] - 0s 553us/sample - loss: 0.6298 - val_loss: 0.8028\n",
      "Epoch 94/100\n",
      "90/90 [==============================] - 0s 499us/sample - loss: 0.6302 - val_loss: 0.8029\n",
      "Epoch 95/100\n",
      "90/90 [==============================] - 0s 578us/sample - loss: 0.6291 - val_loss: 0.8041\n",
      "Epoch 96/100\n",
      "90/90 [==============================] - 0s 490us/sample - loss: 0.6288 - val_loss: 0.8049\n",
      "Epoch 97/100\n",
      "90/90 [==============================] - 0s 494us/sample - loss: 0.6282 - val_loss: 0.8071\n",
      "Epoch 98/100\n",
      "90/90 [==============================] - 0s 479us/sample - loss: 0.6278 - val_loss: 0.8076\n",
      "Epoch 99/100\n",
      "90/90 [==============================] - 0s 525us/sample - loss: 0.6273 - val_loss: 0.8068\n",
      "Epoch 100/100\n",
      "90/90 [==============================] - 0s 492us/sample - loss: 0.6272 - val_loss: 0.8050\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-46d4f929b989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbeddingAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepRegression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeepRegression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbeddingAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepRegression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeepRegression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-2df3dd148cce>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, X, Y, deepRegression)\u001b[0m\n\u001b[1;32m      9\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                   shuffle=True)\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_user_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_embedding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_game_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'game_embedding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "deepRegression = True\n",
    "\n",
    "users = np.array(users)\n",
    "games = np.array(games)\n",
    "ratings = np.array(ratings)\n",
    "\n",
    "if deepRegression:\n",
    "    features = []\n",
    "    for i in range(len(users)):\n",
    "        features.append(env.get_feature_vector(users[i], games[i]))\n",
    "    features = np.float64(features)\n",
    "    agent = EmbeddingAgent([users, games, features], ratings, deepRegression=deepRegression)\n",
    "else:\n",
    "    agent = EmbeddingAgent([users, games], ratings, deepRegression=deepRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user = 25, available games = [0 1 2 5 7 8 9], choosen_game = 2\n",
      "reward = 1\n",
      "\n",
      "user = 19, available games = [0 1 2 3 4 5 6 7 8 9], choosen_game = 7\n",
      "reward = 2\n",
      "\n",
      "user = 10, available games = [2 4 7 8], choosen_game = 2\n",
      "reward = 1\n",
      "\n",
      "user = 13, available games = [0 1 2 3 4 5 7 8 9], choosen_game = 9\n",
      "reward = 3\n",
      "\n",
      "user = 1, available games = [4 5 6 8], choosen_game = 8\n",
      "reward = 2\n",
      "\n",
      "user = 18, available games = [1 2 3 4 5 6 7 8 9], choosen_game = 2\n",
      "reward = 3\n",
      "\n",
      "user = 19, available games = [0 1 2 3 4 5 6 8 9], choosen_game = 8\n",
      "reward = 2\n",
      "\n",
      "user = 19, available games = [0 1 2 3 4 5 6 9], choosen_game = 4\n",
      "reward = 1\n",
      "\n",
      "user = 2, available games = [2 4 8 9], choosen_game = 9\n",
      "reward = 2\n",
      "\n",
      "user = 8, available games = [0 1 4 7 8 9], choosen_game = 9\n",
      "reward = 2\n",
      "\n",
      "user = 6, available games = [0 1 2 3 4 6 9], choosen_game = 9\n",
      "reward = 4\n",
      "\n",
      "user = 4, available games = [0 4 8 9], choosen_game = 9\n",
      "reward = 2\n",
      "\n",
      "user = 10, available games = [4 7 8], choosen_game = 8\n",
      "reward = 1\n",
      "\n",
      "user = 15, available games = [0 2 4 7 9], choosen_game = 9\n",
      "reward = 3\n",
      "\n",
      "user = 6, available games = [0 1 2 3 4 6], choosen_game = 2\n",
      "reward = 2\n",
      "\n",
      "user = 3, available games = [0 2 4 5 6 7 9], choosen_game = 9\n",
      "reward = 2\n",
      "\n",
      "user = 24, available games = [0 1 2 3 6 7 8 9], choosen_game = 9\n",
      "reward = 2\n",
      "\n",
      "user = 3, available games = [0 2 4 5 6 7], choosen_game = 2\n",
      "reward = 1\n",
      "\n",
      "user = 13, available games = [0 1 2 3 4 5 7 8], choosen_game = 2\n",
      "reward = 3\n",
      "\n",
      "user = 11, available games = [0 1 2 3 5 6 7 8 9], choosen_game = 9\n",
      "reward = 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running several trials\n",
    "nb_iteration = 20 #how many trials\n",
    "for i in range(nb_iteration):\n",
    "    user, available_games = env.step()\n",
    "    choosen_game = agent.act(user, available_games)\n",
    "    reward = env.update(user, choosen_game)\n",
    "    rating_matrix[user, choosen_game] = reward\n",
    "    print(\"user = {}, available games = {}, choosen_game = {}\".format(user,available_games,choosen_game))\n",
    "    print(\"reward = {}\\n\".format(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
