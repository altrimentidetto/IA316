{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steam (Environment - Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "# Basic import\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from scipy.stats import norm\n",
    "import pdb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout, Dot, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\" \n",
    "    Contextual Multi-Armed Bandit environment\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_films, nb_users, \n",
    "                 context_size = 2,\n",
    "                 displayed_users_embedding_size = 2, #used for the features vector\n",
    "                 displayed_games_embedding_size = 2, #used for the features vector\n",
    "                 noise_size = 3,\n",
    "                 seed=None):     \n",
    "        self._rng = np.random.RandomState(seed)\n",
    "        #-------------------------------------------------------#\n",
    "        self._nb_games = nb_games\n",
    "        self._nb_users = nb_users\n",
    "        self._p = context_size # size of user, size of game\n",
    "        self._displayed_users_embedding_size = displayed_users_embedding_size\n",
    "        self._displayed_games_embedding_size = displayed_games_embedding_size\n",
    "        self._noise_size = noise_size\n",
    "        #-------------------------------------------------------#\n",
    "        self.user_mean = np.ones(self._p)\n",
    "        self.user_var = np.ones(self._p)\n",
    "        self.game_mean = np.ones(self._p)\n",
    "        self.game_var = np.ones(self._p)\n",
    "        #-------------------------------------------------------#\n",
    "        self.finish = False # flag to know when reset the environment (all games played)\n",
    "    \n",
    "    \n",
    "    def step(self):\n",
    "        if self.finish:\n",
    "            print(\"All games played: reset the environment\")\n",
    "        user = self._rng.randint(0, self._nb_users)\n",
    "        \n",
    "        available_games = np.where(self._available_games[user] == 1)[0]\n",
    "        optimal_reward = np.max(self._reward_matrix[user,available_games])\n",
    "        return user, available_games, optimal_reward\n",
    "    \n",
    "    \n",
    "    def update(self, user, game):\n",
    "        reward = self._reward_matrix[user, game]\n",
    "        self._available_games[user, game] = 0\n",
    "        return reward\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self._users = self._rng.normal(loc=self.user_mean,\n",
    "                                                scale=self.user_var,\n",
    "                                                size=(self._nb_users, self._p))\n",
    "        self._games = self._rng.normal(loc=self.game_mean,\n",
    "                                                scale=self.game_var,\n",
    "                                                size=(self._nb_games, self._p))\n",
    "        \n",
    "        z_mean = self.user_mean.dot(self.game_mean)\n",
    "        z_var = self.user_var.dot(self.game_var) + self.user_var.dot(np.square(self.game_mean)) + \\\n",
    "                self.game_var.dot(np.square(self.user_mean))\n",
    "        z = norm(z_mean, np.sqrt(z_var))\n",
    "        self.z_cut_points = z.ppf([0.2, 0.4, 0.6, 0.8]) # buckets\n",
    "        self._available_games = np.ones((nb_users, nb_games))        \n",
    "        \n",
    "        self._reward_matrix = np.zeros((nb_users, nb_games))\n",
    "        \n",
    "        for i in range(self._reward_matrix.shape[0]):\n",
    "            for j in range(self._reward_matrix.shape[1]):\n",
    "                real_score = self._users[i].dot(self._games[j])\n",
    "                self._reward_matrix[i, j] = np.searchsorted(self.z_cut_points, real_score) + 1\n",
    "\n",
    "        users = deepcopy(self._users)\n",
    "        return users\n",
    "\n",
    "    \n",
    "    def get_feature_vector(self, user, game):\n",
    "        user_embedding = self._users[user]\n",
    "        game_embedding = self._games[game]\n",
    "        \n",
    "        if self._displayed_users_embedding_size + self._displayed_games_embedding_size > 0:\n",
    "            variables = np.array([user_embedding[:self._displayed_users_embedding_size],\n",
    "                                  game_embedding[:self._displayed_games_embedding_size]])\n",
    "\n",
    "            if self._noise_size > 0:\n",
    "                noise = self._rng.normal(loc=np.ones(self._noise_size),\n",
    "                                         scale=np.ones(self._noise_size),\n",
    "                                         size=self._noise_size)\n",
    "                \n",
    "                variables = np.append(variables, noise)\n",
    "                \n",
    "        return variables\n",
    "        \n",
    "        \n",
    "    def reset_seed(self, seed=None):\n",
    "        self._rng = np.random.RandomState(seed)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \"\"\" \n",
    "    Random agent\n",
    "    \"\"\"\n",
    "    def __init__(self, seed = None):\n",
    "        self._rng = np.random.RandomState(seed)\n",
    "    \n",
    "    def act(self, available_games):\n",
    "        action = self._rng.choice(available_games)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic parameter\n",
    "nb_users = 30 #number of users in the context\n",
    "nb_games = 20 #number of games in the context\n",
    "context_size = 2 #number of different film categories = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.76884571,  1.07555227],\n",
       "       [-0.1306297 ,  0.34856983],\n",
       "       [ 0.10688437, -0.27410098],\n",
       "       [ 0.93884557,  1.06451384],\n",
       "       [ 1.41011295,  0.42711751],\n",
       "       [ 0.19866638,  2.31203519],\n",
       "       [ 2.27469887, -0.2143576 ],\n",
       "       [ 1.31371941, -0.44482142],\n",
       "       [ 0.6310387 ,  0.23077342],\n",
       "       [ 1.3926161 ,  1.05729383],\n",
       "       [ 3.08997884,  1.04197131],\n",
       "       [ 0.95165928,  0.48684608],\n",
       "       [ 0.91541072, -0.21545008],\n",
       "       [-0.41293073, -0.48691055],\n",
       "       [ 1.38222486,  1.937673  ],\n",
       "       [ 2.77267804,  1.87882801],\n",
       "       [ 1.33171912,  0.69396433],\n",
       "       [ 2.24026615,  0.78437316],\n",
       "       [ 1.15592948,  1.09805553],\n",
       "       [ 1.83209585,  3.04520542],\n",
       "       [ 0.68318608, -0.31283291],\n",
       "       [-0.75445746,  1.10209408],\n",
       "       [-0.36150208,  1.48178488],\n",
       "       [ 0.79167126,  0.90813649],\n",
       "       [ 1.70268816,  1.10365506],\n",
       "       [ 1.62123638,  1.95411497],\n",
       "       [ 3.03781352,  0.51554878],\n",
       "       [ 1.2071549 ,  2.64424216],\n",
       "       [ 0.5117926 ,  0.98217174],\n",
       "       [ 1.46891556,  1.27987266]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the environment\n",
    "env = Environment(nb_games,nb_users,context_size,seed=2020)\n",
    "env.reset() #reset and initilize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the agent\n",
    "agent = RandomAgent(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the experiment and generate some historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Running several trials\n",
    "nb_iteration = 100 #how many trials\n",
    "users = list()\n",
    "games = list()\n",
    "ratings = list()\n",
    "for i in range(nb_iteration):\n",
    "    user, available_games, _ = env.step()\n",
    "    choosen_game = agent.act(available_games)\n",
    "    reward = env.update(user, choosen_game)\n",
    "    users.append(user)\n",
    "    games.append(choosen_game)\n",
    "    ratings.append(reward)\n",
    "    '''\n",
    "    print(\"user = {}, recommended_games = {}, choosen_game = {}\".format(user,recommended_games,choosen_game))\n",
    "    print(\"reward = {}\\n\".format(reward))\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(Model):\n",
    "    def __init__(self, embedding_size, max_user, max_game):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.user_embedding = Embedding(output_dim=embedding_size,\n",
    "                                        input_dim=max_user,\n",
    "                                        input_length=1,\n",
    "                                        name='user_embedding')\n",
    "        self.game_embedding = Embedding(output_dim=embedding_size,\n",
    "                                        input_dim=max_game,\n",
    "                                        input_length=1,\n",
    "                                        name='game_embedding')\n",
    "        self.flatten = Flatten()\n",
    "        self.dot = Dot(axes=1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        user_inputs = inputs[0]\n",
    "        game_inputs = inputs[1]\n",
    "        \n",
    "        user_vecs = self.flatten(self.user_embedding(user_inputs))\n",
    "        game_vecs = self.flatten(self.game_embedding(game_inputs))\n",
    "        \n",
    "        y = self.dot([user_vecs, game_vecs])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepRegressionModel(Model):\n",
    "\n",
    "    def __init__(self, embedding_size, max_user, max_game):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.user_embedding = Embedding(output_dim=embedding_size,\n",
    "                                        input_dim=max_user,\n",
    "                                        input_length=1,\n",
    "                                        name='user_embedding')\n",
    "        self.game_embedding = Embedding(output_dim=embedding_size,\n",
    "                                        input_dim=max_game,\n",
    "                                        input_length=1,\n",
    "                                        name='game_embedding')\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        self.concat = Concatenate()\n",
    "        \n",
    "        self.dense1 = Dense(16, activation=\"relu\")\n",
    "        self.dense2 = Dense(8, activation=\"relu\")\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        user_inputs = inputs[0]\n",
    "        game_inputs = inputs[1]\n",
    "        feature_inputs = inputs[2]\n",
    "        \n",
    "        user_vecs = self.flatten(self.user_embedding(user_inputs))\n",
    "        game_vecs = self.flatten(self.game_embedding(game_inputs))\n",
    "        \n",
    "        # input_vecs = self.concat([user_vecs, game_vecs, self.flatten(feature_inputs)])\n",
    "        input_vecs = self.concat([user_vecs, game_vecs])\n",
    "        \n",
    "        y = self.dense1(input_vecs)\n",
    "        y = self.dense2(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingAgent:\n",
    "    def __init__(self, X, Y, deepRegression=False):\n",
    "        if deepRegression:\n",
    "            self._model = DeepRegressionModel(64, nb_users, nb_games) ## passare nb\n",
    "        else:\n",
    "            self._model = RegressionModel(64, nb_users, nb_games)\n",
    "        self._model.compile(optimizer=\"adam\", loss='mae')\n",
    "        self._model.fit(X, Y,\n",
    "                  batch_size=64, epochs=100, validation_split=0.1,\n",
    "                  shuffle=True)\n",
    "        self._user_embeddings = self._model.get_weights()[0]\n",
    "        self._game_embeddings = self._model.get_weights()[1]\n",
    "    \n",
    "    def act(self, user, available_games):\n",
    "        user_embedding = self._user_embeddings[user]\n",
    "        dot_products = self._game_embeddings @ user_embedding\n",
    "        user_embedding_norm = np.linalg.norm(user_embedding)\n",
    "        all_item_norms = np.linalg.norm(self._game_embeddings, axis=1)\n",
    "        norm_products = user_embedding_norm * all_item_norms\n",
    "        sims = dot_products / (norm_products)\n",
    "        sims = np.argsort(sims)[::-1]\n",
    "        mask = np.in1d(sims, available_games)\n",
    "        sims = sims[mask]\n",
    "        return sims[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90 samples, validate on 10 samples\n",
      "Epoch 1/100\n",
      "90/90 [==============================] - 0s 5ms/sample - loss: 2.7575 - val_loss: 1.9015\n",
      "Epoch 2/100\n",
      "90/90 [==============================] - 0s 561us/sample - loss: 2.7549 - val_loss: 1.9015\n",
      "Epoch 3/100\n",
      "90/90 [==============================] - 0s 479us/sample - loss: 2.7527 - val_loss: 1.9016\n",
      "Epoch 4/100\n",
      "90/90 [==============================] - 0s 466us/sample - loss: 2.7506 - val_loss: 1.9016\n",
      "Epoch 5/100\n",
      "90/90 [==============================] - 0s 453us/sample - loss: 2.7485 - val_loss: 1.9016\n",
      "Epoch 6/100\n",
      "90/90 [==============================] - 0s 455us/sample - loss: 2.7464 - val_loss: 1.9016\n",
      "Epoch 7/100\n",
      "90/90 [==============================] - 0s 475us/sample - loss: 2.7442 - val_loss: 1.9016\n",
      "Epoch 8/100\n",
      "90/90 [==============================] - 0s 508us/sample - loss: 2.7420 - val_loss: 1.9016\n",
      "Epoch 9/100\n",
      "90/90 [==============================] - 0s 445us/sample - loss: 2.7398 - val_loss: 1.9016\n",
      "Epoch 10/100\n",
      "90/90 [==============================] - 0s 457us/sample - loss: 2.7374 - val_loss: 1.9016\n",
      "Epoch 11/100\n",
      "90/90 [==============================] - 0s 458us/sample - loss: 2.7350 - val_loss: 1.9015\n",
      "Epoch 12/100\n",
      "90/90 [==============================] - 0s 474us/sample - loss: 2.7324 - val_loss: 1.9014\n",
      "Epoch 13/100\n",
      "90/90 [==============================] - 0s 447us/sample - loss: 2.7297 - val_loss: 1.9013\n",
      "Epoch 14/100\n",
      "90/90 [==============================] - 0s 457us/sample - loss: 2.7269 - val_loss: 1.9011\n",
      "Epoch 15/100\n",
      "90/90 [==============================] - 0s 450us/sample - loss: 2.7238 - val_loss: 1.9009\n",
      "Epoch 16/100\n",
      "90/90 [==============================] - 0s 493us/sample - loss: 2.7206 - val_loss: 1.9006\n",
      "Epoch 17/100\n",
      "90/90 [==============================] - 0s 485us/sample - loss: 2.7172 - val_loss: 1.9003\n",
      "Epoch 18/100\n",
      "90/90 [==============================] - 0s 505us/sample - loss: 2.7135 - val_loss: 1.8999\n",
      "Epoch 19/100\n",
      "90/90 [==============================] - 0s 468us/sample - loss: 2.7097 - val_loss: 1.8994\n",
      "Epoch 20/100\n",
      "90/90 [==============================] - 0s 455us/sample - loss: 2.7055 - val_loss: 1.8989\n",
      "Epoch 21/100\n",
      "90/90 [==============================] - 0s 463us/sample - loss: 2.7011 - val_loss: 1.8983\n",
      "Epoch 22/100\n",
      "90/90 [==============================] - 0s 547us/sample - loss: 2.6964 - val_loss: 1.8975\n",
      "Epoch 23/100\n",
      "90/90 [==============================] - 0s 600us/sample - loss: 2.6914 - val_loss: 1.8967\n",
      "Epoch 24/100\n",
      "90/90 [==============================] - 0s 534us/sample - loss: 2.6860 - val_loss: 1.8958\n",
      "Epoch 25/100\n",
      "90/90 [==============================] - 0s 511us/sample - loss: 2.6803 - val_loss: 1.8948\n",
      "Epoch 26/100\n",
      "90/90 [==============================] - 0s 552us/sample - loss: 2.6744 - val_loss: 1.8937\n",
      "Epoch 27/100\n",
      "90/90 [==============================] - 0s 542us/sample - loss: 2.6679 - val_loss: 1.8925\n",
      "Epoch 28/100\n",
      "90/90 [==============================] - 0s 580us/sample - loss: 2.6612 - val_loss: 1.8912\n",
      "Epoch 29/100\n",
      "90/90 [==============================] - 0s 499us/sample - loss: 2.6541 - val_loss: 1.8897\n",
      "Epoch 30/100\n",
      "90/90 [==============================] - 0s 581us/sample - loss: 2.6465 - val_loss: 1.8882\n",
      "Epoch 31/100\n",
      "90/90 [==============================] - 0s 595us/sample - loss: 2.6384 - val_loss: 1.8865\n",
      "Epoch 32/100\n",
      "90/90 [==============================] - 0s 808us/sample - loss: 2.6300 - val_loss: 1.8846\n",
      "Epoch 33/100\n",
      "90/90 [==============================] - 0s 461us/sample - loss: 2.6211 - val_loss: 1.8825\n",
      "Epoch 34/100\n",
      "90/90 [==============================] - 0s 496us/sample - loss: 2.6116 - val_loss: 1.8802\n",
      "Epoch 35/100\n",
      "90/90 [==============================] - 0s 518us/sample - loss: 2.6017 - val_loss: 1.8778\n",
      "Epoch 36/100\n",
      "90/90 [==============================] - 0s 492us/sample - loss: 2.5912 - val_loss: 1.8751\n",
      "Epoch 37/100\n",
      "90/90 [==============================] - 0s 469us/sample - loss: 2.5803 - val_loss: 1.8721\n",
      "Epoch 38/100\n",
      "90/90 [==============================] - 0s 564us/sample - loss: 2.5687 - val_loss: 1.8689\n",
      "Epoch 39/100\n",
      "90/90 [==============================] - 0s 511us/sample - loss: 2.5566 - val_loss: 1.8654\n",
      "Epoch 40/100\n",
      "90/90 [==============================] - 0s 463us/sample - loss: 2.5440 - val_loss: 1.8617\n",
      "Epoch 41/100\n",
      "90/90 [==============================] - 0s 551us/sample - loss: 2.5307 - val_loss: 1.8578\n",
      "Epoch 42/100\n",
      "90/90 [==============================] - 0s 634us/sample - loss: 2.5169 - val_loss: 1.8536\n",
      "Epoch 43/100\n",
      "90/90 [==============================] - 0s 569us/sample - loss: 2.5025 - val_loss: 1.8491\n",
      "Epoch 44/100\n",
      "90/90 [==============================] - 0s 595us/sample - loss: 2.4875 - val_loss: 1.8443\n",
      "Epoch 45/100\n",
      "90/90 [==============================] - 0s 544us/sample - loss: 2.4716 - val_loss: 1.8393\n",
      "Epoch 46/100\n",
      "90/90 [==============================] - 0s 562us/sample - loss: 2.4555 - val_loss: 1.8342\n",
      "Epoch 47/100\n",
      "90/90 [==============================] - 0s 548us/sample - loss: 2.4385 - val_loss: 1.8287\n",
      "Epoch 48/100\n",
      "90/90 [==============================] - 0s 520us/sample - loss: 2.4209 - val_loss: 1.8229\n",
      "Epoch 49/100\n",
      "90/90 [==============================] - 0s 555us/sample - loss: 2.4027 - val_loss: 1.8168\n",
      "Epoch 50/100\n",
      "90/90 [==============================] - 0s 541us/sample - loss: 2.3836 - val_loss: 1.8104\n",
      "Epoch 51/100\n",
      "90/90 [==============================] - 0s 578us/sample - loss: 2.3641 - val_loss: 1.8037\n",
      "Epoch 52/100\n",
      "90/90 [==============================] - 0s 537us/sample - loss: 2.3439 - val_loss: 1.7968\n",
      "Epoch 53/100\n",
      "90/90 [==============================] - 0s 587us/sample - loss: 2.3227 - val_loss: 1.7896\n",
      "Epoch 54/100\n",
      "90/90 [==============================] - 0s 685us/sample - loss: 2.3009 - val_loss: 1.7821\n",
      "Epoch 55/100\n",
      "90/90 [==============================] - 0s 508us/sample - loss: 2.2786 - val_loss: 1.7744\n",
      "Epoch 56/100\n",
      "90/90 [==============================] - 0s 581us/sample - loss: 2.2555 - val_loss: 1.7665\n",
      "Epoch 57/100\n",
      "90/90 [==============================] - 0s 542us/sample - loss: 2.2315 - val_loss: 1.7582\n",
      "Epoch 58/100\n",
      "90/90 [==============================] - 0s 508us/sample - loss: 2.2068 - val_loss: 1.7494\n",
      "Epoch 59/100\n",
      "90/90 [==============================] - 0s 581us/sample - loss: 2.1814 - val_loss: 1.7403\n",
      "Epoch 60/100\n",
      "90/90 [==============================] - 0s 450us/sample - loss: 2.1554 - val_loss: 1.7310\n",
      "Epoch 61/100\n",
      "90/90 [==============================] - 0s 554us/sample - loss: 2.1284 - val_loss: 1.7216\n",
      "Epoch 62/100\n",
      "90/90 [==============================] - 0s 462us/sample - loss: 2.1007 - val_loss: 1.7116\n",
      "Epoch 63/100\n",
      "90/90 [==============================] - 0s 448us/sample - loss: 2.0724 - val_loss: 1.7010\n",
      "Epoch 64/100\n",
      "90/90 [==============================] - 0s 508us/sample - loss: 2.0433 - val_loss: 1.6901\n",
      "Epoch 65/100\n",
      "90/90 [==============================] - 0s 422us/sample - loss: 2.0134 - val_loss: 1.6788\n",
      "Epoch 66/100\n",
      "90/90 [==============================] - 0s 473us/sample - loss: 1.9828 - val_loss: 1.6670\n",
      "Epoch 67/100\n",
      "90/90 [==============================] - 0s 513us/sample - loss: 1.9520 - val_loss: 1.6549\n",
      "Epoch 68/100\n",
      "90/90 [==============================] - 0s 450us/sample - loss: 1.9218 - val_loss: 1.6425\n",
      "Epoch 69/100\n",
      "90/90 [==============================] - 0s 516us/sample - loss: 1.8930 - val_loss: 1.6299\n",
      "Epoch 70/100\n",
      "90/90 [==============================] - 0s 465us/sample - loss: 1.8631 - val_loss: 1.6175\n",
      "Epoch 71/100\n",
      "90/90 [==============================] - 0s 482us/sample - loss: 1.8337 - val_loss: 1.6050\n",
      "Epoch 72/100\n",
      "90/90 [==============================] - 0s 467us/sample - loss: 1.8036 - val_loss: 1.5925\n",
      "Epoch 73/100\n",
      "90/90 [==============================] - 0s 440us/sample - loss: 1.7737 - val_loss: 1.5798\n",
      "Epoch 74/100\n",
      "90/90 [==============================] - 0s 470us/sample - loss: 1.7445 - val_loss: 1.5672\n",
      "Epoch 75/100\n",
      "90/90 [==============================] - 0s 456us/sample - loss: 1.7173 - val_loss: 1.5559\n",
      "Epoch 76/100\n",
      "90/90 [==============================] - 0s 510us/sample - loss: 1.6894 - val_loss: 1.5460\n",
      "Epoch 77/100\n",
      "90/90 [==============================] - 0s 477us/sample - loss: 1.6617 - val_loss: 1.5367\n",
      "Epoch 78/100\n",
      "90/90 [==============================] - 0s 452us/sample - loss: 1.6328 - val_loss: 1.5279\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 485us/sample - loss: 1.6042 - val_loss: 1.5194\n",
      "Epoch 80/100\n",
      "90/90 [==============================] - 0s 433us/sample - loss: 1.5742 - val_loss: 1.5108\n",
      "Epoch 81/100\n",
      "90/90 [==============================] - 0s 587us/sample - loss: 1.5454 - val_loss: 1.5022\n",
      "Epoch 82/100\n",
      "90/90 [==============================] - 0s 417us/sample - loss: 1.5152 - val_loss: 1.4939\n",
      "Epoch 83/100\n",
      "90/90 [==============================] - 0s 454us/sample - loss: 1.4846 - val_loss: 1.4855\n",
      "Epoch 84/100\n",
      "90/90 [==============================] - 0s 469us/sample - loss: 1.4541 - val_loss: 1.4769\n",
      "Epoch 85/100\n",
      "90/90 [==============================] - 0s 439us/sample - loss: 1.4231 - val_loss: 1.4681\n",
      "Epoch 86/100\n",
      "90/90 [==============================] - 0s 484us/sample - loss: 1.3913 - val_loss: 1.4591\n",
      "Epoch 87/100\n",
      "90/90 [==============================] - 0s 439us/sample - loss: 1.3591 - val_loss: 1.4498\n",
      "Epoch 88/100\n",
      "90/90 [==============================] - 0s 477us/sample - loss: 1.3274 - val_loss: 1.4410\n",
      "Epoch 89/100\n",
      "90/90 [==============================] - 0s 424us/sample - loss: 1.2960 - val_loss: 1.4318\n",
      "Epoch 90/100\n",
      "90/90 [==============================] - 0s 459us/sample - loss: 1.2642 - val_loss: 1.4222\n",
      "Epoch 91/100\n",
      "90/90 [==============================] - 0s 423us/sample - loss: 1.2325 - val_loss: 1.4124\n",
      "Epoch 92/100\n",
      "90/90 [==============================] - 0s 450us/sample - loss: 1.2024 - val_loss: 1.4028\n",
      "Epoch 93/100\n",
      "90/90 [==============================] - 0s 423us/sample - loss: 1.1723 - val_loss: 1.3938\n",
      "Epoch 94/100\n",
      "90/90 [==============================] - 0s 437us/sample - loss: 1.1453 - val_loss: 1.3855\n",
      "Epoch 95/100\n",
      "90/90 [==============================] - 0s 456us/sample - loss: 1.1205 - val_loss: 1.3776\n",
      "Epoch 96/100\n",
      "90/90 [==============================] - 0s 607us/sample - loss: 1.0968 - val_loss: 1.3708\n",
      "Epoch 97/100\n",
      "90/90 [==============================] - 0s 423us/sample - loss: 1.0736 - val_loss: 1.3643\n",
      "Epoch 98/100\n",
      "90/90 [==============================] - 0s 490us/sample - loss: 1.0505 - val_loss: 1.3577\n",
      "Epoch 99/100\n",
      "90/90 [==============================] - 0s 436us/sample - loss: 1.0278 - val_loss: 1.3514\n",
      "Epoch 100/100\n",
      "90/90 [==============================] - 0s 451us/sample - loss: 1.0045 - val_loss: 1.3454\n"
     ]
    }
   ],
   "source": [
    "deepRegression = False\n",
    "\n",
    "users = np.array(users)\n",
    "games = np.array(games)\n",
    "ratings = np.array(ratings)\n",
    "\n",
    "if deepRegression:\n",
    "    features = []\n",
    "    for i in range(len(users)):\n",
    "        features.append(env.get_feature_vector(users[i], games[i]))\n",
    "    features = np.float64(features)\n",
    "    agent = EmbeddingAgent([users, games, features], ratings, deepRegression=deepRegression)\n",
    "else:\n",
    "    agent = EmbeddingAgent([users, games], ratings, deepRegression=deepRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_iteration = 40\n",
    "nb_exp = 100\n",
    "#---------------#\n",
    "prev_env = deepcopy(env)\n",
    "regret = np.zeros(nb_exp)\n",
    "cum_regret = np.zeros((nb_exp, nb_iteration))\n",
    "\n",
    "for t in range(nb_exp):\n",
    "    env = deepcopy(prev_env)\n",
    "    env.reset_seed()\n",
    "    regrets = np.zeros(nb_iteration)\n",
    "    for i in range(nb_iteration):\n",
    "        user, available_games, optimal_reward = env.step()\n",
    "        choosen_game = agent.act(user, available_games)\n",
    "        reward = env.update(user, choosen_game)\n",
    "        regrets[i] = optimal_reward - reward\n",
    "        # print(\"user = {}, available games = {}, choosen_game = {}\".format(user,available_games,choosen_game))\n",
    "        # print(\"reward = {}\\n\".format(reward))\n",
    "    cum_regret[t] = np.cumsum(regrets)\n",
    "    regret[t] = np.sum(regrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cum_regret.mean(axis=0), color='blue')\n",
    "plt.plot(np.quantile(cum_regret, 0.05,axis=0), color='grey', alpha=0.5)\n",
    "plt.plot(np.quantile(cum_regret, 0.95,axis=0), color='grey', alpha=0.5)\n",
    "plt.title('Mean regret: {:.2f}'.format(regret.mean()))\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('regret')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
