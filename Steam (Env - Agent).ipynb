{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steam (Environment - Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "# Basic import\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from scipy.stats import norm\n",
    "import pdb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout, Dot, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\" \n",
    "    Contextual Multi-Armed Bandit environment\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_films, nb_users, \n",
    "                 context_size = 2,\n",
    "                 displayed_users_embedding_size = 2, #used for the features vector\n",
    "                 displayed_games_embedding_size = 2, #used for the features vector\n",
    "                 noise_size = 3,\n",
    "                 seed=None):     \n",
    "        \n",
    "        self._rng = np.random.RandomState(seed)\n",
    "        #-------------------------------------------------------#\n",
    "        \n",
    "        self._nb_games = nb_games\n",
    "        self._nb_users = nb_users\n",
    "        self._p = context_size # size of user, size of game\n",
    "        self._displayed_users_embedding_size = displayed_users_embedding_size\n",
    "        self._displayed_games_embedding_size = displayed_games_embedding_size\n",
    "        self._noise_size = noise_size\n",
    "   \n",
    "        #-------------------------------------------------------#\n",
    "    \n",
    "        self.user_mean = np.ones(self._p)\n",
    "        self.user_var = np.ones(self._p)\n",
    "        self.game_mean = np.ones(self._p)\n",
    "        self.game_var = np.ones(self._p)\n",
    "        \n",
    "        #-------------------------------------------------------#\n",
    "        self.finish = False # flag to know when reset the environment (all games played)\n",
    "    \n",
    "    def step(self):\n",
    "        \n",
    "        if self._available_games.sum() == 0:#if all players played all games\n",
    "            self.finish = True\n",
    "            print(\"All games played reset the environment\")\n",
    "            return 0,0, self.finish\n",
    "            \n",
    "        \"\"\" Choose a game \"\"\"\n",
    "        user = self.get_next_user()#always a user that have at least one gama still to play\n",
    "        \n",
    "        \n",
    "        available_games = np.where(self._available_games[user] == 1)[0]\n",
    "        \n",
    "\n",
    "        \n",
    "        return user, available_games, self.finish\n",
    "    \n",
    "    def get_next_user(self):\n",
    "        \n",
    "        user = self._rng.randint(0, self._nb_users)\n",
    "        \n",
    "        if np.sum(self._available_games[user,:]) > 0:#still some games to play\n",
    "            return user\n",
    "        else:#all games played for the current user--> change and find a random one between the one who still have some games\n",
    "            row,cols = np.where(self._available_games == 1)\n",
    "            #pdb.set_trace()\n",
    "            ret = self._rng.choice(row)\n",
    "            return ret\n",
    "    \n",
    "    def update(self, user, game):\n",
    "        reward = self._reward_matrix[user, game]\n",
    "        self._available_games[user, game] = 0\n",
    "        return reward\n",
    "    \n",
    "    def reset(self):\n",
    "        self.finish = False\n",
    "        self._users = self._rng.normal(loc=self.user_mean,\n",
    "                                                scale=self.user_var,\n",
    "                                                size=(self._nb_users, self._p))\n",
    "        self._games = self._rng.normal(loc=self.game_mean,\n",
    "                                                scale=self.game_var,\n",
    "                                                size=(self._nb_games, self._p))\n",
    "        \n",
    "        z_mean = self.user_mean.dot(self.game_mean)\n",
    "        z_var = self.user_var.dot(self.game_var) + self.user_var.dot(np.square(self.game_mean)) + \\\n",
    "                self.game_var.dot(np.square(self.user_mean))\n",
    "        z = norm(z_mean, np.sqrt(z_var))\n",
    "        self.z_cut_points = z.ppf([0.2, 0.4, 0.6, 0.8]) # buckets\n",
    "        self._available_games = np.ones((nb_users, nb_games))        \n",
    "        \n",
    "        self._reward_matrix = np.zeros((nb_users, nb_games))\n",
    "        \n",
    "        for i in range(self._reward_matrix.shape[0]):\n",
    "            for j in range(self._reward_matrix.shape[1]):\n",
    "                real_score = self._users[i].dot(self._games[j])\n",
    "                self._reward_matrix[i, j] = np.searchsorted(self.z_cut_points, real_score) + 1\n",
    "\n",
    "        users = deepcopy(self._users)\n",
    "        return users\n",
    "\n",
    "    def get_feature_vector(self, user, game):\n",
    "        user_embedding = self._users[user]\n",
    "        game_embedding = self._games[game]\n",
    "        \n",
    "        if self._displayed_users_embedding_size + self._displayed_games_embedding_size > 0:\n",
    "            variables = np.array([user_embedding[:self._displayed_users_embedding_size],\n",
    "                                  game_embedding[:self._displayed_games_embedding_size]])\n",
    "\n",
    "            if self._noise_size > 0:\n",
    "                noise = self._rng.normal(loc=np.ones(self._noise_size),\n",
    "                                         scale=np.ones(self._noise_size),\n",
    "                                         size=self._noise_size)\n",
    "                \n",
    "                variables = np.append(variables, noise)\n",
    "                \n",
    "            return variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \"\"\" \n",
    "    Random agent\n",
    "    \"\"\"\n",
    "    def __init__(self, seed = None):\n",
    "        self._rng = np.random.RandomState(seed)\n",
    "    \n",
    "    def act(self, available_games):\n",
    "        action = self._rng.choice(available_games)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic parameter\n",
    "nb_users = 30 #number of users in the context\n",
    "nb_games = 10 #number of games in the context\n",
    "context_size = 2 #number of different film categories = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.76884571,  1.07555227],\n",
       "       [-0.1306297 ,  0.34856983],\n",
       "       [ 0.10688437, -0.27410098],\n",
       "       [ 0.93884557,  1.06451384],\n",
       "       [ 1.41011295,  0.42711751],\n",
       "       [ 0.19866638,  2.31203519],\n",
       "       [ 2.27469887, -0.2143576 ],\n",
       "       [ 1.31371941, -0.44482142],\n",
       "       [ 0.6310387 ,  0.23077342],\n",
       "       [ 1.3926161 ,  1.05729383],\n",
       "       [ 3.08997884,  1.04197131],\n",
       "       [ 0.95165928,  0.48684608],\n",
       "       [ 0.91541072, -0.21545008],\n",
       "       [-0.41293073, -0.48691055],\n",
       "       [ 1.38222486,  1.937673  ],\n",
       "       [ 2.77267804,  1.87882801],\n",
       "       [ 1.33171912,  0.69396433],\n",
       "       [ 2.24026615,  0.78437316],\n",
       "       [ 1.15592948,  1.09805553],\n",
       "       [ 1.83209585,  3.04520542],\n",
       "       [ 0.68318608, -0.31283291],\n",
       "       [-0.75445746,  1.10209408],\n",
       "       [-0.36150208,  1.48178488],\n",
       "       [ 0.79167126,  0.90813649],\n",
       "       [ 1.70268816,  1.10365506],\n",
       "       [ 1.62123638,  1.95411497],\n",
       "       [ 3.03781352,  0.51554878],\n",
       "       [ 1.2071549 ,  2.64424216],\n",
       "       [ 0.5117926 ,  0.98217174],\n",
       "       [ 1.46891556,  1.27987266]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the environment\n",
    "env = Environment(nb_games,nb_users,context_size,seed=2020)\n",
    "env.reset() #reset and initilize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the agent\n",
    "agent = RandomAgent(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the experiment and generate some historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300.0\n",
      "299.0\n",
      "298.0\n",
      "297.0\n",
      "296.0\n",
      "295.0\n",
      "294.0\n",
      "293.0\n",
      "292.0\n",
      "291.0\n",
      "290.0\n",
      "289.0\n",
      "288.0\n",
      "287.0\n",
      "286.0\n",
      "285.0\n",
      "284.0\n",
      "283.0\n",
      "282.0\n",
      "281.0\n",
      "280.0\n",
      "279.0\n",
      "278.0\n",
      "277.0\n",
      "276.0\n",
      "275.0\n",
      "274.0\n",
      "273.0\n",
      "272.0\n",
      "271.0\n",
      "270.0\n",
      "269.0\n",
      "268.0\n",
      "267.0\n",
      "266.0\n",
      "265.0\n",
      "264.0\n",
      "263.0\n",
      "262.0\n",
      "261.0\n",
      "260.0\n",
      "259.0\n",
      "258.0\n",
      "257.0\n",
      "256.0\n",
      "255.0\n",
      "254.0\n",
      "253.0\n",
      "252.0\n",
      "251.0\n",
      "250.0\n",
      "249.0\n",
      "248.0\n",
      "247.0\n",
      "246.0\n",
      "245.0\n",
      "244.0\n",
      "243.0\n",
      "242.0\n",
      "241.0\n",
      "240.0\n",
      "239.0\n",
      "238.0\n",
      "237.0\n",
      "236.0\n",
      "235.0\n",
      "234.0\n",
      "233.0\n",
      "232.0\n",
      "231.0\n",
      "230.0\n",
      "229.0\n",
      "228.0\n",
      "227.0\n",
      "226.0\n",
      "225.0\n",
      "224.0\n",
      "223.0\n",
      "222.0\n",
      "221.0\n",
      "220.0\n",
      "219.0\n",
      "218.0\n",
      "217.0\n",
      "216.0\n",
      "215.0\n",
      "214.0\n",
      "213.0\n",
      "212.0\n",
      "211.0\n",
      "210.0\n",
      "209.0\n",
      "208.0\n",
      "207.0\n",
      "206.0\n",
      "205.0\n",
      "204.0\n",
      "203.0\n",
      "202.0\n",
      "201.0\n",
      "200.0\n",
      "199.0\n",
      "198.0\n",
      "197.0\n",
      "196.0\n",
      "195.0\n",
      "194.0\n",
      "193.0\n",
      "192.0\n",
      "191.0\n",
      "190.0\n",
      "189.0\n",
      "188.0\n",
      "187.0\n",
      "186.0\n",
      "185.0\n",
      "184.0\n",
      "183.0\n",
      "182.0\n",
      "181.0\n",
      "180.0\n",
      "179.0\n",
      "178.0\n",
      "177.0\n",
      "176.0\n",
      "175.0\n",
      "174.0\n",
      "173.0\n",
      "172.0\n",
      "171.0\n",
      "170.0\n",
      "169.0\n",
      "168.0\n",
      "167.0\n",
      "166.0\n",
      "165.0\n",
      "164.0\n",
      "163.0\n",
      "162.0\n",
      "161.0\n",
      "160.0\n",
      "159.0\n",
      "158.0\n",
      "157.0\n",
      "156.0\n",
      "155.0\n",
      "154.0\n",
      "153.0\n",
      "152.0\n",
      "151.0\n",
      "150.0\n",
      "149.0\n",
      "148.0\n",
      "147.0\n",
      "146.0\n",
      "145.0\n",
      "144.0\n",
      "143.0\n",
      "142.0\n",
      "141.0\n",
      "140.0\n",
      "139.0\n",
      "138.0\n",
      "137.0\n",
      "136.0\n",
      "135.0\n",
      "134.0\n",
      "133.0\n",
      "132.0\n",
      "131.0\n",
      "130.0\n",
      "129.0\n",
      "128.0\n",
      "127.0\n",
      "126.0\n",
      "125.0\n",
      "124.0\n",
      "123.0\n",
      "122.0\n",
      "121.0\n",
      "120.0\n",
      "119.0\n",
      "118.0\n",
      "117.0\n",
      "116.0\n",
      "115.0\n",
      "114.0\n",
      "113.0\n",
      "112.0\n",
      "111.0\n",
      "110.0\n",
      "109.0\n",
      "108.0\n",
      "107.0\n",
      "106.0\n",
      "105.0\n",
      "104.0\n",
      "103.0\n",
      "102.0\n",
      "101.0\n",
      "100.0\n",
      "99.0\n",
      "98.0\n",
      "97.0\n",
      "96.0\n",
      "95.0\n",
      "94.0\n",
      "93.0\n",
      "92.0\n",
      "91.0\n",
      "90.0\n",
      "89.0\n",
      "88.0\n",
      "87.0\n",
      "86.0\n",
      "85.0\n",
      "84.0\n",
      "83.0\n",
      "82.0\n",
      "81.0\n",
      "80.0\n",
      "79.0\n",
      "78.0\n",
      "77.0\n",
      "76.0\n",
      "75.0\n",
      "74.0\n",
      "73.0\n",
      "72.0\n",
      "71.0\n",
      "70.0\n",
      "69.0\n",
      "68.0\n",
      "67.0\n",
      "66.0\n",
      "65.0\n",
      "64.0\n",
      "63.0\n",
      "62.0\n",
      "61.0\n",
      "60.0\n",
      "59.0\n",
      "58.0\n",
      "57.0\n",
      "56.0\n",
      "55.0\n",
      "54.0\n",
      "53.0\n",
      "52.0\n",
      "51.0\n",
      "50.0\n",
      "49.0\n",
      "48.0\n",
      "47.0\n",
      "46.0\n",
      "45.0\n",
      "44.0\n",
      "43.0\n",
      "42.0\n",
      "41.0\n",
      "40.0\n",
      "39.0\n",
      "38.0\n",
      "37.0\n",
      "36.0\n",
      "35.0\n",
      "34.0\n",
      "33.0\n",
      "32.0\n",
      "31.0\n",
      "30.0\n",
      "29.0\n",
      "28.0\n",
      "27.0\n",
      "26.0\n",
      "25.0\n",
      "24.0\n",
      "23.0\n",
      "22.0\n",
      "21.0\n",
      "20.0\n",
      "19.0\n",
      "18.0\n",
      "17.0\n",
      "16.0\n",
      "15.0\n",
      "14.0\n",
      "13.0\n",
      "12.0\n",
      "11.0\n",
      "10.0\n",
      "9.0\n",
      "8.0\n",
      "7.0\n",
      "6.0\n",
      "5.0\n",
      "4.0\n",
      "3.0\n",
      "2.0\n",
      "1.0\n",
      "0.0\n",
      "All games played reset the environment\n",
      "Maybe too many trial try to reduce and reset the environment\n",
      "rating matrix: \n",
      " [[2. 2. 1. 3. 1. 2. 2. 2. 2. 1.]\n",
      " [2. 2. 1. 2. 1. 2. 2. 2. 2. 1.]\n",
      " [1. 1. 2. 1. 2. 1. 1. 1. 1. 2.]\n",
      " [2. 3. 3. 2. 4. 3. 2. 3. 3. 2.]\n",
      " [2. 3. 4. 2. 5. 3. 1. 3. 3. 2.]\n",
      " [2. 3. 1. 4. 2. 4. 3. 4. 5. 1.]\n",
      " [2. 3. 5. 1. 5. 3. 1. 3. 3. 3.]\n",
      " [2. 2. 4. 1. 5. 2. 1. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 3. 2. 1. 2. 2. 2.]\n",
      " [2. 3. 3. 2. 5. 4. 2. 3. 4. 2.]\n",
      " [3. 4. 5. 2. 5. 5. 1. 4. 5. 3.]\n",
      " [2. 2. 3. 2. 4. 3. 1. 3. 3. 2.]\n",
      " [2. 2. 3. 1. 4. 2. 1. 2. 2. 2.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [2. 4. 3. 3. 5. 4. 2. 4. 5. 2.]\n",
      " [3. 5. 5. 3. 5. 5. 1. 5. 5. 3.]\n",
      " [2. 3. 3. 2. 5. 3. 1. 3. 3. 2.]\n",
      " [2. 4. 5. 2. 5. 4. 1. 4. 4. 3.]\n",
      " [2. 3. 3. 2. 4. 3. 2. 3. 4. 2.]\n",
      " [3. 5. 4. 4. 5. 5. 2. 5. 5. 2.]\n",
      " [2. 2. 3. 1. 3. 2. 1. 2. 2. 2.]\n",
      " [2. 2. 1. 3. 1. 2. 2. 2. 2. 1.]\n",
      " [2. 2. 1. 3. 1. 3. 2. 3. 3. 1.]\n",
      " [2. 3. 3. 2. 3. 3. 2. 3. 3. 2.]\n",
      " [2. 3. 4. 2. 5. 4. 1. 4. 4. 2.]\n",
      " [3. 4. 4. 3. 5. 4. 2. 4. 5. 2.]\n",
      " [2. 4. 5. 1. 5. 4. 1. 4. 4. 3.]\n",
      " [3. 4. 3. 4. 4. 5. 2. 5. 5. 2.]\n",
      " [2. 3. 2. 2. 3. 3. 2. 3. 3. 2.]\n",
      " [2. 3. 4. 2. 5. 4. 2. 4. 4. 2.]]\n"
     ]
    }
   ],
   "source": [
    "# Running several trials\n",
    "nb_iteration = 10000 #how many trials\n",
    "rating_matrix = np.zeros((env._nb_users, env._nb_games))\n",
    "users = list()\n",
    "games = list()\n",
    "ratings = list()\n",
    "for i in range(nb_iteration):\n",
    "    user, available_games, finish = env.step()\n",
    "    if finish:\n",
    "        print(\"Maybe too many trial try to reduce and reset the environment\")\n",
    "        break\n",
    "    choosen_game = agent.act(available_games)\n",
    "    reward = env.update(user, choosen_game)\n",
    "    users.append(user)\n",
    "    games.append(choosen_game)\n",
    "    ratings.append(reward)\n",
    "    rating_matrix[user, choosen_game] = reward\n",
    "    '''\n",
    "    print(\"user = {}, recommended_games = {}, choosen_game = {}\".format(user,recommended_games,choosen_game))\n",
    "    print(\"reward = {}\\n\".format(reward))\n",
    "    '''\n",
    "    \n",
    "print(\"rating matrix: \\n\", str(rating_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(Model):\n",
    "    def __init__(self, embedding_size, max_user, max_game):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.user_embedding = Embedding(output_dim=embedding_size,\n",
    "                                        input_dim=max_user,\n",
    "                                        input_length=1,\n",
    "                                        name='user_embedding')\n",
    "        self.game_embedding = Embedding(output_dim=embedding_size,\n",
    "                                        input_dim=max_game,\n",
    "                                        input_length=1,\n",
    "                                        name='game_embedding')\n",
    "        self.flatten = Flatten()\n",
    "        self.dot = Dot(axes=1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        user_inputs = inputs[0]\n",
    "        game_inputs = inputs[1]\n",
    "        \n",
    "        user_vecs = self.flatten(self.user_embedding(user_inputs))\n",
    "        game_vecs = self.flatten(self.game_embedding(game_inputs))\n",
    "        \n",
    "        y = self.dot([user_vecs, game_vecs])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepRegressionModel(Model):\n",
    "\n",
    "    def __init__(self, embedding_size, max_user, max_game):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.user_embedding = Embedding(output_dim=embedding_size,\n",
    "                                        input_dim=max_user,\n",
    "                                        input_length=1,\n",
    "                                        name='user_embedding')\n",
    "        self.game_embedding = Embedding(output_dim=embedding_size,\n",
    "                                        input_dim=max_game,\n",
    "                                        input_length=1,\n",
    "                                        name='game_embedding')\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        self.concat = Concatenate()\n",
    "        \n",
    "        self.dense1 = Dense(16, activation=\"relu\")\n",
    "        self.dense2 = Dense(8, activation=\"relu\")\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        user_inputs = inputs[0]\n",
    "        game_inputs = inputs[1]\n",
    "        feature_inputs = inputs[2]\n",
    "        \n",
    "        user_vecs = self.flatten(self.user_embedding(user_inputs))\n",
    "        game_vecs = self.flatten(self.game_embedding(game_inputs))\n",
    "        \n",
    "        # input_vecs = self.concat([user_vecs, game_vecs, self.flatten(feature_inputs)])\n",
    "        input_vecs = self.concat([user_vecs, game_vecs])\n",
    "        \n",
    "        y = self.dense1(input_vecs)\n",
    "        y = self.dense2(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingAgent:\n",
    "    def __init__(self, X, Y, deepRegression=False):\n",
    "        if deepRegression:\n",
    "            self._model = DeepRegressionModel(64, nb_users, nb_games) ## passare nb\n",
    "        else:\n",
    "            self._model = RegressionModel(64, nb_users, nb_games)\n",
    "        self._model.compile(optimizer=\"adam\", loss='mae')\n",
    "        self._model.fit(X, Y,\n",
    "                  batch_size=64, epochs=100, validation_split=0.1,\n",
    "                  shuffle=True)\n",
    "        self._user_embeddings = self._model.get_weights()[0]\n",
    "        self._game_embeddings = self._model.get_weights()[1]\n",
    "    \n",
    "    def act(self, user, available_games):\n",
    "        user_embedding = self._user_embeddings[user]\n",
    "        dot_products = self._game_embeddings @ user_embedding\n",
    "        user_embedding_norm = np.linalg.norm(user_embedding)\n",
    "        all_item_norms = np.linalg.norm(self._game_embeddings, axis=1)\n",
    "        norm_products = user_embedding_norm * all_item_norms\n",
    "        sims = dot_products / (norm_products)\n",
    "        sims = np.argsort(sims)[::-1]\n",
    "        mask = np.in1d(sims, available_games)\n",
    "        sims = sims[mask]\n",
    "        return sims[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90 samples, validate on 10 samples\n",
      "Epoch 1/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.7678 - val_loss: 2.9975\n",
      "Epoch 2/100\n",
      "90/90 [==============================] - 0s 810us/sample - loss: 2.7656 - val_loss: 2.9975\n",
      "Epoch 3/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.7636 - val_loss: 2.9975\n",
      "Epoch 4/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.7617 - val_loss: 2.9973\n",
      "Epoch 5/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.7598 - val_loss: 2.9972\n",
      "Epoch 6/100\n",
      "90/90 [==============================] - 0s 857us/sample - loss: 2.7578 - val_loss: 2.9970\n",
      "Epoch 7/100\n",
      "90/90 [==============================] - 0s 648us/sample - loss: 2.7558 - val_loss: 2.9967\n",
      "Epoch 8/100\n",
      "90/90 [==============================] - 0s 681us/sample - loss: 2.7537 - val_loss: 2.9964\n",
      "Epoch 9/100\n",
      "90/90 [==============================] - 0s 798us/sample - loss: 2.7515 - val_loss: 2.9960\n",
      "Epoch 10/100\n",
      "90/90 [==============================] - 0s 670us/sample - loss: 2.7491 - val_loss: 2.9956\n",
      "Epoch 11/100\n",
      "90/90 [==============================] - 0s 659us/sample - loss: 2.7466 - val_loss: 2.9950\n",
      "Epoch 12/100\n",
      "90/90 [==============================] - 0s 658us/sample - loss: 2.7440 - val_loss: 2.9943\n",
      "Epoch 13/100\n",
      "90/90 [==============================] - 0s 640us/sample - loss: 2.7412 - val_loss: 2.9935\n",
      "Epoch 14/100\n",
      "90/90 [==============================] - 0s 671us/sample - loss: 2.7381 - val_loss: 2.9926\n",
      "Epoch 15/100\n",
      "90/90 [==============================] - 0s 665us/sample - loss: 2.7349 - val_loss: 2.9915\n",
      "Epoch 16/100\n",
      "90/90 [==============================] - 0s 667us/sample - loss: 2.7314 - val_loss: 2.9903\n",
      "Epoch 17/100\n",
      "90/90 [==============================] - 0s 691us/sample - loss: 2.7276 - val_loss: 2.9890\n",
      "Epoch 18/100\n",
      "90/90 [==============================] - 0s 656us/sample - loss: 2.7236 - val_loss: 2.9874\n",
      "Epoch 19/100\n",
      "90/90 [==============================] - 0s 648us/sample - loss: 2.7192 - val_loss: 2.9857\n",
      "Epoch 20/100\n",
      "90/90 [==============================] - 0s 683us/sample - loss: 2.7146 - val_loss: 2.9838\n",
      "Epoch 21/100\n",
      "90/90 [==============================] - 0s 702us/sample - loss: 2.7096 - val_loss: 2.9817\n",
      "Epoch 22/100\n",
      "90/90 [==============================] - 0s 787us/sample - loss: 2.7041 - val_loss: 2.9794\n",
      "Epoch 23/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.6984 - val_loss: 2.9769\n",
      "Epoch 24/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.6921 - val_loss: 2.9740\n",
      "Epoch 25/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.6855 - val_loss: 2.9710\n",
      "Epoch 26/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.6784 - val_loss: 2.9677\n",
      "Epoch 27/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.6707 - val_loss: 2.9641\n",
      "Epoch 28/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.6627 - val_loss: 2.9602\n",
      "Epoch 29/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.6540 - val_loss: 2.9560\n",
      "Epoch 30/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.6449 - val_loss: 2.9515\n",
      "Epoch 31/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.6352 - val_loss: 2.9465\n",
      "Epoch 32/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.6250 - val_loss: 2.9413\n",
      "Epoch 33/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.6140 - val_loss: 2.9356\n",
      "Epoch 34/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.6024 - val_loss: 2.9295\n",
      "Epoch 35/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.5903 - val_loss: 2.9232\n",
      "Epoch 36/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.5775 - val_loss: 2.9164\n",
      "Epoch 37/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.5639 - val_loss: 2.9093\n",
      "Epoch 38/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.5498 - val_loss: 2.9017\n",
      "Epoch 39/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.5348 - val_loss: 2.8938\n",
      "Epoch 40/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.5191 - val_loss: 2.8853\n",
      "Epoch 41/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.5027 - val_loss: 2.8763\n",
      "Epoch 42/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.4854 - val_loss: 2.8669\n",
      "Epoch 43/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.4674 - val_loss: 2.8573\n",
      "Epoch 44/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.4486 - val_loss: 2.8472\n",
      "Epoch 45/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.4290 - val_loss: 2.8366\n",
      "Epoch 46/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.4086 - val_loss: 2.8255\n",
      "Epoch 47/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.3872 - val_loss: 2.8137\n",
      "Epoch 48/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.3650 - val_loss: 2.8013\n",
      "Epoch 49/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.3422 - val_loss: 2.7882\n",
      "Epoch 50/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.3182 - val_loss: 2.7743\n",
      "Epoch 51/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.2935 - val_loss: 2.7598\n",
      "Epoch 52/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.2677 - val_loss: 2.7448\n",
      "Epoch 53/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.2411 - val_loss: 2.7291\n",
      "Epoch 54/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.2139 - val_loss: 2.7130\n",
      "Epoch 55/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.1854 - val_loss: 2.6961\n",
      "Epoch 56/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.1560 - val_loss: 2.6784\n",
      "Epoch 57/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.1260 - val_loss: 2.6602\n",
      "Epoch 58/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.0949 - val_loss: 2.6412\n",
      "Epoch 59/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.0628 - val_loss: 2.6214\n",
      "Epoch 60/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 2.0298 - val_loss: 2.6014\n",
      "Epoch 61/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.9956 - val_loss: 2.5813\n",
      "Epoch 62/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.9607 - val_loss: 2.5606\n",
      "Epoch 63/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.9247 - val_loss: 2.5394\n",
      "Epoch 64/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.8875 - val_loss: 2.5172\n",
      "Epoch 65/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.8496 - val_loss: 2.4944\n",
      "Epoch 66/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.8108 - val_loss: 2.4707\n",
      "Epoch 67/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.7728 - val_loss: 2.4469\n",
      "Epoch 68/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.7352 - val_loss: 2.4223\n",
      "Epoch 69/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.6982 - val_loss: 2.3972\n",
      "Epoch 70/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.6609 - val_loss: 2.3726\n",
      "Epoch 71/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.6274 - val_loss: 2.3487\n",
      "Epoch 72/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.5938 - val_loss: 2.3252\n",
      "Epoch 73/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.5629 - val_loss: 2.3013\n",
      "Epoch 74/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.5289 - val_loss: 2.2779\n",
      "Epoch 75/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.4937 - val_loss: 2.2543\n",
      "Epoch 76/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.4581 - val_loss: 2.2304\n",
      "Epoch 77/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.4231 - val_loss: 2.2061\n",
      "Epoch 78/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.3868 - val_loss: 2.1813\n",
      "Epoch 79/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.3484 - val_loss: 2.1557\n",
      "Epoch 80/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.3113 - val_loss: 2.1297\n",
      "Epoch 81/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.2757 - val_loss: 2.1039\n",
      "Epoch 82/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.2389 - val_loss: 2.0778\n",
      "Epoch 83/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.2059 - val_loss: 2.0517\n",
      "Epoch 84/100\n",
      "90/90 [==============================] - 0s 845us/sample - loss: 1.1726 - val_loss: 2.0256\n",
      "Epoch 85/100\n",
      "90/90 [==============================] - 0s 840us/sample - loss: 1.1397 - val_loss: 1.9986\n",
      "Epoch 86/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.1081 - val_loss: 1.9718\n",
      "Epoch 87/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.0794 - val_loss: 1.9447\n",
      "Epoch 88/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.0515 - val_loss: 1.9182\n",
      "Epoch 89/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.0280 - val_loss: 1.8921\n",
      "Epoch 90/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 1.0022 - val_loss: 1.8678\n",
      "Epoch 91/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 0.9790 - val_loss: 1.8443\n",
      "Epoch 92/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 0.9551 - val_loss: 1.8219\n",
      "Epoch 93/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 0.9336 - val_loss: 1.7996\n",
      "Epoch 94/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 0.9120 - val_loss: 1.7771\n",
      "Epoch 95/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 0.8915 - val_loss: 1.7541\n",
      "Epoch 96/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 0.8712 - val_loss: 1.7312\n",
      "Epoch 97/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 0.8520 - val_loss: 1.7082\n",
      "Epoch 98/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 0.8335 - val_loss: 1.6859\n",
      "Epoch 99/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 0.8151 - val_loss: 1.6642\n",
      "Epoch 100/100\n",
      "90/90 [==============================] - 0s 1ms/sample - loss: 0.7957 - val_loss: 1.6416\n"
     ]
    }
   ],
   "source": [
    "deepRegression = False\n",
    "\n",
    "users = np.array(users)\n",
    "games = np.array(games)\n",
    "ratings = np.array(ratings)\n",
    "\n",
    "if deepRegression:\n",
    "    features = []\n",
    "    for i in range(len(users)):\n",
    "        features.append(env.get_feature_vector(users[i], games[i]))\n",
    "    features = np.float64(features)\n",
    "    agent = EmbeddingAgent([users, games, features], ratings, deepRegression=deepRegression)\n",
    "else:\n",
    "    agent = EmbeddingAgent([users, games], ratings, deepRegression=deepRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user = 24, available games = [1 2 3 4 5 6 7 8], choosen_game = 8\n",
      "reward = 4.0\n",
      "\n",
      "user = 12, available games = [1 3 5 6 7 8 9], choosen_game = 6\n",
      "reward = 1.0\n",
      "\n",
      "user = 23, available games = [0 1 3 4 5 7 8 9], choosen_game = 5\n",
      "reward = 3.0\n",
      "\n",
      "user = 26, available games = [0 1 2 3 4 7 9], choosen_game = 1\n",
      "reward = 4.0\n",
      "\n",
      "user = 26, available games = [0 2 3 4 7 9], choosen_game = 2\n",
      "reward = 5.0\n",
      "\n",
      "user = 29, available games = [0 4 5 6 8 9], choosen_game = 8\n",
      "reward = 4.0\n",
      "\n",
      "user = 1, available games = [0 1 5 6 8], choosen_game = 0\n",
      "reward = 2.0\n",
      "\n",
      "user = 8, available games = [3 4 6], choosen_game = 3\n",
      "reward = 2.0\n",
      "\n",
      "user = 7, available games = [0 4 6 7 8], choosen_game = 8\n",
      "reward = 2.0\n",
      "\n",
      "user = 12, available games = [1 3 5 7 8 9], choosen_game = 5\n",
      "reward = 2.0\n",
      "\n",
      "user = 5, available games = [0 2 4 5 7], choosen_game = 7\n",
      "reward = 4.0\n",
      "\n",
      "user = 27, available games = [2 5], choosen_game = 2\n",
      "reward = 3.0\n",
      "\n",
      "user = 14, available games = [1 2 3 4 5 6 9], choosen_game = 1\n",
      "reward = 4.0\n",
      "\n",
      "user = 15, available games = [0 1 2 3 4 5 6 7 9], choosen_game = 7\n",
      "reward = 5.0\n",
      "\n",
      "user = 29, available games = [0 4 5 6 9], choosen_game = 9\n",
      "reward = 2.0\n",
      "\n",
      "user = 17, available games = [0 2 3 4 5 6 8 9], choosen_game = 8\n",
      "reward = 4.0\n",
      "\n",
      "user = 20, available games = [0 1 2 4 5 7 9], choosen_game = 4\n",
      "reward = 3.0\n",
      "\n",
      "user = 15, available games = [0 1 2 3 4 5 6 9], choosen_game = 1\n",
      "reward = 5.0\n",
      "\n",
      "user = 5, available games = [0 2 4 5], choosen_game = 2\n",
      "reward = 1.0\n",
      "\n",
      "user = 21, available games = [1 3 4 5 6 9], choosen_game = 9\n",
      "reward = 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running several trials\n",
    "nb_iteration = 20 #how many trials\n",
    "for i in range(nb_iteration):\n",
    "    user, available_games, finish = env.step()\n",
    "    if finish:\n",
    "        print(\"Maybe too many trial try to reduce and reset the environment\")\n",
    "        break\n",
    "    choosen_game = agent.act(user, available_games)\n",
    "    reward = env.update(user, choosen_game)\n",
    "    rating_matrix[user, choosen_game] = reward\n",
    "    print(\"user = {}, available games = {}, choosen_game = {}\".format(user,available_games,choosen_game))\n",
    "    print(\"reward = {}\\n\".format(reward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
