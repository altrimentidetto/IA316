{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Bandits\n",
    "Here we create a very basic Agent and Environment we will choose a film among the ones proposed, at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic import\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "from scipy.stats import invgamma, gamma\n",
    "from scipy.stats import t as student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \"\"\" Random agent. \"\"\"\n",
    "    def __init__(self, seed = None):\n",
    "        self._rng = np.random.RandomState(seed)\n",
    "    \n",
    "    def act(self, user_id, recommended_films):\n",
    "        action = self._rng.choice(recommended_films)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CMAB:\n",
    "    \"\"\" \n",
    "    Contextual Multi-Armed Bandit environment with bernoulli rewards.\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_films, nb_users, context_size, seed=None):\n",
    "        \"\"\" mean_reward_a = coef_{0,a} + \\sum_{j,a} x_{j,a}*beta_{j,a} \"\"\"\n",
    "        self._nb_films = nb_films\n",
    "        self._nb_users = nb_users\n",
    "        self._p = context_size # size of user, size of film\n",
    "        self._rng = np.random.RandomState(seed)\n",
    "        self._films = self._rng.uniform(size=(nb_films, context_size))\n",
    "        self._users = self._rng.uniform(size=(nb_users, context_size))\n",
    "        self._reward_matrix = np.zeros((nb_users, nb_films))\n",
    "        for i in range(self._reward_matrix.shape[0]):\n",
    "            for j in range(self._reward_matrix.shape[1]):\n",
    "                film_norm = np.linalg.norm(self._films[j])\n",
    "                user_norm = np.linalg.norm(self._users[i])\n",
    "                reward = np.linalg.norm(self._films[j] - self._users[i], ord=2) \n",
    "                self._reward_matrix[i, j] = reward\n",
    "        self._reward_matrix = (self._reward_matrix / np.max(self._reward_matrix) * 4).astype(int) + 1\n",
    "        self._available_films = np.ones((nb_users, nb_films))\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\" Play an action \"\"\"\n",
    "        user = self._rng.randint(0, self._nb_users)\n",
    "        available_films = np.where(self._available_films[user] == 1)[0]\n",
    "        return user, available_films\n",
    "    \n",
    "    def update(self, user, film):\n",
    "        reward = self._reward_matrix[user, film]\n",
    "        self._available_films[user, film] = 0\n",
    "        return reward\n",
    "    \n",
    "    def reset(self):\n",
    "        self._users = self._rng.uniform(size=(self._nb_users, self._p))\n",
    "        self._reward_matrix = np.zeros((nb_users, nb_films))\n",
    "        for i in range(self._reward_matrix.shape[0]):\n",
    "            for j in range(self._reward_matrix.shape[1]):\n",
    "                film_norm = np.linalg.norm(self._films[j])\n",
    "                user_norm = np.linalg.norm(self._users[i])\n",
    "                reward = np.linalg.norm(self._films[j] - self._users[i], ord=2) \n",
    "                self._reward_matrix[i, j] = reward\n",
    "        self._reward_matrix = (self._reward_matrix / np.max(self._reward_matrix) * 4).astype(int) + 1\n",
    "        self._available_films = np.ones((nb_users, nb_films))\n",
    "        users = deepcopy(self._users)\n",
    "        return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic parameter\n",
    "\n",
    "nb_users = 10 #number of users in the context\n",
    "nb_films = 10 #number of films in the context\n",
    "nb_features = 2 #number of different film categories\n",
    "nb_iteration = 20 #how many trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99950726, 0.92374656],\n",
       "       [0.27108366, 0.8205857 ],\n",
       "       [0.26191241, 0.38855628],\n",
       "       [0.76975835, 0.13069826],\n",
       "       [0.70841285, 0.49818746],\n",
       "       [0.64278088, 0.74700825],\n",
       "       [0.04899137, 0.28122565],\n",
       "       [0.43999946, 0.48934559],\n",
       "       [0.58861554, 0.80239152],\n",
       "       [0.60564912, 0.48580851]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the environment\n",
    "\n",
    "env = CMAB(nb_films,nb_users,nb_features)\n",
    "env.reset() #reset and initilize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the agent\n",
    "\n",
    "agent = RandomAgent(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 9]\n",
      "user_id =7, recommended_films = [0 1 2 3 4 5 6 7 9], choosen_film = 9\n",
      "reward = 1\n",
      "\n",
      "[1 2 3 5 6 7 8 9]\n",
      "user_id =0, recommended_films = [1 2 3 5 6 7 8 9], choosen_film = 8\n",
      "reward = 4\n",
      "\n",
      "[1 2 5 6 8]\n",
      "user_id =9, recommended_films = [1 2 5 6 8], choosen_film = 8\n",
      "reward = 3\n",
      "\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "user_id =5, recommended_films = [0 1 2 3 4 5 6 7 8 9], choosen_film = 1\n",
      "reward = 1\n",
      "\n",
      "[0 1 2 3 4 5 6 8 9]\n",
      "user_id =2, recommended_films = [0 1 2 3 4 5 6 8 9], choosen_film = 1\n",
      "reward = 3\n",
      "\n",
      "[0 2 3 4 5 6 8 9]\n",
      "user_id =2, recommended_films = [0 2 3 4 5 6 8 9], choosen_film = 9\n",
      "reward = 1\n",
      "\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "user_id =8, recommended_films = [0 1 2 3 4 5 6 7 8 9], choosen_film = 5\n",
      "reward = 2\n",
      "\n",
      "[0 1 2 3 4 7 9]\n",
      "user_id =3, recommended_films = [0 1 2 3 4 7 9], choosen_film = 2\n",
      "reward = 4\n",
      "\n",
      "[0 1 2 3 4 6 7 8 9]\n",
      "user_id =8, recommended_films = [0 1 2 3 4 6 7 8 9], choosen_film = 6\n",
      "reward = 2\n",
      "\n",
      "[1 2 5 6]\n",
      "user_id =9, recommended_films = [1 2 5 6], choosen_film = 2\n",
      "reward = 2\n",
      "\n",
      "[0 1 2 3 4 7 8 9]\n",
      "user_id =8, recommended_films = [0 1 2 3 4 7 8 9], choosen_film = 2\n",
      "reward = 1\n",
      "\n",
      "[1 2 3 5 6 7 9]\n",
      "user_id =0, recommended_films = [1 2 3 5 6 7 9], choosen_film = 9\n",
      "reward = 4\n",
      "\n",
      "[1 5 6]\n",
      "user_id =9, recommended_films = [1 5 6], choosen_film = 6\n",
      "reward = 1\n",
      "\n",
      "[0 1 2 3 4 5 6 7]\n",
      "user_id =7, recommended_films = [0 1 2 3 4 5 6 7], choosen_film = 6\n",
      "reward = 1\n",
      "\n",
      "[0 1 2 3 4 5 7]\n",
      "user_id =7, recommended_films = [0 1 2 3 4 5 7], choosen_film = 5\n",
      "reward = 3\n",
      "\n",
      "[0 1 3 4 7 8 9]\n",
      "user_id =8, recommended_films = [0 1 3 4 7 8 9], choosen_film = 7\n",
      "reward = 2\n",
      "\n",
      "[0 2 3 4 5 6 8]\n",
      "user_id =2, recommended_films = [0 2 3 4 5 6 8], choosen_film = 8\n",
      "reward = 3\n",
      "\n",
      "[0 1 2 4 5 6 7 8 9]\n",
      "user_id =4, recommended_films = [0 1 2 4 5 6 7 8 9], choosen_film = 5\n",
      "reward = 2\n",
      "\n",
      "[2 3 6 7 8]\n",
      "user_id =1, recommended_films = [2 3 6 7 8], choosen_film = 6\n",
      "reward = 3\n",
      "\n",
      "[0 1 2 4 6 7 8 9]\n",
      "user_id =4, recommended_films = [0 1 2 4 6 7 8 9], choosen_film = 4\n",
      "reward = 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#running several trials\n",
    "\n",
    "for i in range(nb_iteration):\n",
    "    user_id,recommended_films = env.step()\n",
    "    print(recommended_films)\n",
    "    choosen_film = agent.act(user_id,recommended_films)\n",
    "    print(\"user_id ={}, recommended_films = {}, choosen_film = {}\".format(user_id,recommended_films,choosen_film))\n",
    "    reward = env.update(user_id, choosen_film)\n",
    "    print(\"reward = {}\\n\".format(reward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
